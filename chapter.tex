\documentclass{svmult}

\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{hyperref}

\usepackage[numbers]{natbib}
%\usepackage[switch]{lineno}

\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{enumerate}
\usepackage{paralist} %for inline lists

\usepackage{array}
\usepackage{amsmath}

\usepackage{subfigure}

\usepackage{pseudocode}
\usepackage{alltt}

%diagrams
\usepackage{tikz}
\usetikzlibrary{shapes,trees}

\usepackage{lscape}

\usepackage[draft]{fixme}
\fxusetheme{color}

%%%%%%%%%%%% Custom macros %%%%%%%%%%%%%

%Name of the speaker in a chat
\newcommand{\chatN}[1]{{\footnotesize \textsf{#1}}}
\newcommand{\concept}[1]{{\footnotesize \texttt{#1}}}

\newcommand{\stmt}[1]{{\footnotesize $\langle$\stmttt#1\relax$\rangle$}}
\newcommand{\rawstmt}[1]{{\footnotesize \stmttt#1\relax}}
\def\stmttt#1 #2 #3\relax{{\tt#1} {\bf{\tt #2}} {\tt #3}}

\newcommand{\setstmt}[1]{{\footnotesize [\setstmttt#1\relax]}}
\def\setstmttt#1,#2\relax{\rawstmt{#1}, \rawstmt{#2}}

\newcommand{\ie}{{\textit{i.e.~}}}
\newcommand{\cf}{{\textit{cf~}}}
\newcommand{\eg}{{\textit{e.g.~}}}

%------------------------------------------------------------------------- 

\begin{document}
%\linenumbers

% Alternate row background in tables
\rowcolors{2}{gray!10}{white}

\title*{Towards Grounding Human-Robot Interaction}

\author{
Séverin Lemaignan,
Rachid Alami,
Amit Kumar Pandey,
Matthieu Warnier,
Julien Guitton
}
\authorrunning{Séverin Lemaignan et al.}

\institute{ 
	Séverin Lemaignan, \and Rachid Alami \and Amit Kumar Pandey \and Matthieu Warnier \and Julien Guitton
	\at
	CNRS - LAAS, 7 avenue du Colonel Roche, F-31077 Toulouse, France\\
	Université de Toulouse, UPS, INSA, INP, ISAE, LAAS, F-31077 Toulouse, France\\
	\email{surname.name@laas.fr}
}

\maketitle

\listoffixmes

\clearpage

\setcounter{minitocdepth}{2}
\dominitoc


\begin{abstract}
Abstract. 
\end{abstract}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Grounding Human Interaction}

%\subsection{Situated speech acts}
%\label{intro_example}

{\em A messy table, covered with cardboard boxes, books, video tapes... Thomas
is moving out and is packing everything with the help of Jido, his robot.
``Jido, give me that'', says Thomas while looking at a box that contains a
video tape. The robot smoothly grasps the tape, and hands it to him.}

\begin{figure}%[!ht] 
	\centering
	\includegraphics[width=1.0\linewidth]{figs/pt.jpg} 
	\caption{Interacting with
	the robot in an everyday setup: the human asks for help in vague terms, the
	robot takes into account the human's spatial perspective to refine its
	understanding of the question.} 
	\label{fig|vpt} 
\end{figure}


While this kind of interaction (Fig.~\ref{fig|vpt}) should hopefully sound
quite familiar in a foreseeable future, our robots are not yet quite up to the
task, neither regarding natural language understanding nor plan-making and
manipulation.  To be combined together, those abilities require an unambiguous
and shared representation of concepts (objects, agents, actions...) underlying
the interaction: what are the prerequisites for such a sentence --``Jido, give
me that''-- to be understood by the robot, correctly interpreted in the spatial
context of the interaction, and eventually transformed into an action?

The first step is to understand the meaning of the sentence. To this end, we
must acquire the sentence, convert it into a useful syntactic form (probably
through speech recognition), and understand the semantics of the sentence, \ie
What is referred by ``\textit{Jido}''? What is ``\textit{give}''? What is
``\textit{me}''? And ``\textit{that}''?

Working in a situated context, we need to \emph{resolve} these semantics atoms,
\ie ground them in the sensory-motor space of the robot. For instance,
``\textit{that}'' is a demonstrative pronoun that refers in this context to the
object the human is focusing on.

The next step is to extract and understand the \emph{intended meaning} of the
utterance as thought by the agent. In our example, Thomas obviously wants an
action to be performed by the robot. The action parametrization is conveyed by
the semantics attached to the words and the grammatical structure of the
sentence. In our example, the type of action is given by the verb
``\textit{give}''. Assuming the robot has some procedural knowledge attached to
this symbol, the action type can be considered as grounded for the robot. We
can as well understand that the recipient of the action is the human, the
performer is the robot itself, and the object acted upon is the tape. The
recipient, performer and object are three of the \emph{thematic
roles}~\cite{Gruber1965} that qualify the \emph{give} action. They are
necessary to fully ground the sentence\footnote{This analysis has been inspired
on the work of Austin et al.~\cite{Austin1962}, where this type of sentences
correspond to \emph{speech acts}, comprising of \emph{locutionary act} (the
meaning), \emph{illocutionary} (the intent) and possibly \emph{perlocutionary
acts} (implicit speaker's expectation).}.


\subsection*{A typical human-robot interaction problem}
\label{sec:problem}

\fxfatal{Merge the section with previous one}

Let us consider a robot which is supposed to achieve interactive
object manipulation, fetch and carry tasks and similar tasks in a
domestic environment. The problem we are dealing with here is the
following. Given:
\begin {itemize}
\item a joint goal, which has been previously established and agreed
  upon (through a process which is out of the scope of this paper),
\item the current situation, acquired through perception or
  deduction from previous perceptions, including the state of the
  environment of the robot and of the human,
\end {itemize}
the robot controller computes an action to execute and who (the 
human or the robot, or both in case of a joint action) has to perform
it, and then controls or monitors its execution. The operation
continues until the goal is achieved, is declared unachievable or is
abandoned by the human.

To do so, the robot has to be equipped with a number of decisional, planning
and interpretation abilities where its human partner is taken
explicitly into account. It needs to be able:
\begin {itemize}
\item to build and maintain relevant robot and human beliefs
  (from the robot perspective) with respect to state of the world and the task,
\item to build and maintain iteratively shared (human-robot) plans, 
\item to refine and execute the actions it has to perform, and to monitor 
those achieved by its human partner.
\end {itemize}

Besides, we would like to build such abilities in a generic way, and
to provide several levels of parametrization allowing to adapt to
various environments, and various levels of involvement of the robot
ranging from teammate behavior to assistant or proactive helper.

\subsection{Approach}

\begin{figure}[htb]
\centering
\includegraphics[width=9cm]{figs/grounding_robot.pdf}
\caption{Robot reasoning about HRI and anticipation of human activities:
  sources of information are multi-modal dialogue, and observation of
  environment and human activity}
\label{fig|hri-dec}
\end{figure}

We envision HRI in a context where two agents (a human and a robot)
share a common space and exchange information through various
modalities. Our aim is to endow the robot with an explicit
consideration of the human and with the ability to manage its
interactions with him (Figure~\ref{fig|hri-dec}). This must be
considered at the architecture level as well as at the task/motion
planning and execution level. 

\fxfatal{rewrite this paragraph}


\emph{Physical environment modeling} and \emph{spatial reasoning}
(grouped under the term \emph{situation assessment}): this component is in
charge of building and maintaining a coherent model of the physical world. This
model is realistic in the sense that it relies on accurate 3D models of both
manipulable objects and humans. It also has dedicated mechanisms to manage
disappearing or occluded objects.  The geometric model is used to compute
several spatial properties of the scene that actually convert the original
sensory data into symbolic beliefs, including relative locations of objects,
visibility state, gestures (such as pointing), etc.  Assuming that other agents
are also represented in the model, the same computations are applied to analyze
the scene from each agent's point of view (\ie from their \emph{perspectives}).

\emph{Knowledge representation and management}: the robot is endowed with
an active knowledge base that provides a symbolic model of its beliefs of the
world, as well as models for each cognitive agent the robot interacts with.
Used in combination with the situation assessment framework, this proves an
essential feature (\cite{Roy2005,Kruijff2010}) to enable perspective-aware
grounding of natural language.

Our platform relies on OWL-DL\footnote{\emph{Web Ontology Language -
Description Logics}, a decidable subset of the first-order logics,
\url{http://www.w3.org/TR/owl2-primer/}} ontologies and features continuous
storage, querying and event triggering over the pool of facts known by the
robot.

\subsubsection{An architecture for grounding}
\label{sec:Framework}


Interaction happens as a consequence of an explicit request of the
human to satisfy a goal or because the robot finds itself in a
situation where it is useful if not mandatory. In both cases, the
robot has a goal to satisfy.  An important issue is the notion of
engagement, a process in which the robot will have to establish,
maintain and terminate a connection with a human partner. 
This covers goal establishment, selection of an incremental refinement
of the task that is intended to be achieved, and execution control
including monitoring, and even influencing, human task performance and
his/her commitment to the goal. The human involvement may range from a
direct participation to the task achievement, to a simple
``acceptance'' of robot activity in his/her close vicinity.
 
\begin{figure*}[thpb]
  \centering
  \includegraphics[width=1.0\textwidth]{./figs/architecture-overview.pdf} \\
  \caption {Architecture of the robot control system}
  \label{architecture_fg}
\end{figure*}
 
Our robot is controlled by a three layer
architecture~\cite{Alami1998}.  We present briefly its decisional
layer. The proposed decisional framework consists of several entities,
having each a specific role as illustrated by
Figure~\ref{architecture_fg}.  We describe how the robot is controlled
through an analysis of the three main activities performed by the
robot controller:

\begin {enumerate}
	\item Situation assessment and context management 
	\item Goals and plans management
	\item Action refinement, execution and monitoring
\end {enumerate}

The next three sections describe the three robot controller
activities and how they make use of a number of key components in the
architecture:

\fxfatal{Completely rewrite this section. Mention Dialogs as a first class citizen!}

\begin{itemize}
\item SPARK: Spatial Reasoning and Knowledge module \cite{Sisbot2011}
\item ORO: a knowledge management module \cite{Lemaignan2010}
\item HATP: a Human-Aware Task Planner \cite{Alili2008}
\item A set of Human aware motion, placement and manipulation planners
 \cite{Sisbot2008, Mainprice2011, Pandey2010}
\end{itemize}


\subsubsection{Symbolic Facts and Beliefs Management}

The facts produced by the geometric and temporal reasoning component are stored
in a central symbolic knowledge base, called ORO. This key component is
describe at length in section~\ref{cognitivekernel}. We briefly introduce here
its main interaction channels with the other components, in particular those
related to geometric and temporal reasoning.

\fxfatal{To do!}


\subsection{Related Work}

Our work builds on top of years of work in the artificial intelligence
community around the symbol grounding issue: we can mention
Searle~\cite{Searle1980} who introduces it with the \emph{Chinese room}
metaphor, Harnard~\cite{Harnad1990} who coins the following (slightly
paraphrased in the robotic context) definition of symbol grounding:
\emph{``make the meanings of the meaningless symbol tokens intrinsic to the
robotic system"}, Coradeschi and Saffioti~\cite{Coradeschi2003} who focus on
robotics and define the meaning of \emph{anchoring} as \emph{``the process of
creating and maintaining the correspondence between symbols and sensor data
that refer to the same physical object''} and Ziemke~\cite{Ziemke1999} who
elaborates on the need of embodiment for symbol grounding.

Our contribution relates to two narrower fields: natural language in embodied
interaction contexts and knowledge acquisition and representation in robotic
systems.

Processing natural language in situated contexts is already an established
research field. In~\cite{Roy2005}, Roy and Reiter summarize what they see as
the main challenges to be tackled: cross-modal representation systems,
association of words with perceptual and action categories, modeling of
context, figuring out the right granularity of models, integrating temporal
modeling and planning, ability to match past (learned) experiences with the
current interaction and ability to take into account the human perspective.
This list offers an interesting entry point to evaluate our contribution.

Kruijff et al. provides in~\cite{Kruijff2010} an up-to-date survey of
literature on situated human-robot dialogue, focusing on formal representation
systems, bi-directionality of the interaction and context building. They point
out as well that compared to the cognitive psychology community, the ``situated
AI'' community started only recently to take into account agents' focus of
attention, perspective and temporal projection abilities.

Dialogue processing in real robots have been explored by several teams.  Brick
and Scheutz~\cite{Brick2007} have contributions regarding natural language
processing in an incremental way, and how this enables instant back-channel
feedback (like nodding). Hüwel et al.~\cite{Huwel2006} propose the concept of
\textit{Situated Semantic Unit}: atoms are extracted from sentences exposing
semantic links to other units. The parser tries to satisfy these links and
rates the semantic interpretation of the sentence. Used in conjunction with
ontologies, their approach offers robustness to ungrammatical or partial
utterances. They validated the approach with an extensive user-study.

Zender et al.~\cite{Zender2009} address the generation of referring expressions
(GRE~\cite{Dale1995}) in situated dialogue for topological knowledge.  They consider
both the reference resolution and reference description tasks, and rely on
OWL-DL representation and SPARQL\footnote{{\em SPARQL Protocol and RDF Query
Language}, \url{http://www.w3.org/TR/rdf-sparql-query/}} to extract
\emph{topological contexts} from their knowledge base.

While mostly implemented on virtual agents, the GLAIR cognitive architecture by
Shapiro and Bona~\cite{Shapiro2009} is an architecture explicitly built to
tackle the grounding issue from the percept to the decision. It is a
three-layers architecture: a \emph{Knowledge Layer}, a low-level
\emph{Sensori-Actuator Layer} and an intermediate \emph{Perceptuo-Motor Layer}
that binds the previous two.  The knowledge layer relies on a custom knowledge
representation language (more expressive than first-order logic), and natural
language processing capabilities similar to ours are available. The GLAIR
project has been only demonstrated in a limited set of environments, but
exhibits interesting features such as explicit management of contexts of facts
and memory models (long term/short term, episodic/semantic).

Also worth mentioning, Mavridis and Roy~\cite{Mavridis2005} propose the idea of
a \emph{grounded situation model} which is an amodal model of the world where
different sensing modalities, including verbal ones (the robot is able to
\emph{imagine} objects), are merged. Their framework also allows the management of
the interaction history (the human can ask for a past event). They propose an
implementation in an environment built on simple entities (a manipulator arm
and colored balls).

In the field of symbolic knowledge processing for robots, Gunderson and
Gunderson~\cite{Gunderson2008} introduce the concept of \emph{reification}
(based on both recognition and pre-afference) as an intermediate step between
pattern recognition and symbol grounding. Their underlying storage of knowledge
relies on ontologies and bio-inspired memory models. While sharing similar
foundations to our work, their proposal is based on fairly simple perceptual
modalities and does not develop complex symbolic models that could enable
human-robot interaction.

Suh et al.~\cite{Suh2007} develop {\sc OMRKF}, an ontology-based reasoning
framework for robotics. They tackle the grounding problem by storing low-level
facts (like SIFT visual features) in a layered symbolic architecture that works
well in simple sensori-motor spaces. However this approach raises concerns
regarding scalability and management of more complex entities or interactions.

Daoutis et al.~\cite{Daoutis2009} introduce one of the first complete
architectures for grounded human-robot interaction. They successfully bind
low-level percepts (including view-point independent SIFT based object
recognition) to a high-level knowledge representation and reasoning system.
They base their knowledge model directly on the \textit{ResearchCyc} ontology
(including the \textit{MicroTheories} concept), used in combination with the
{\sc CycL} language. This enables second-order logic modeling and access to a
large common-sense knowledge base.

Beetz et al.~\cite{Beetz2010} proposes a cognitive architecture called
\textsc{CRAM} (Cognitive Robot Abstract Machine) that integrates
\textsc{KnowRob}~\cite{Tenorth2009a}, a knowledge processing framework based on
Prolog. Its underlying storage is based on an OWL ontology, derived from
\textsc{OpenCyc}. \textsc{CRAM} and \textsc{KownRob} have been demonstrated on
several real-world scenarios, where natural language recipes extracted from
Internet had to be translated into plans and executed in a kitchen environment,
perceived and rebuilt on-line by the robots. While Prolog offers more flexible
modeling (no constraints on the arity of predicate, where Description Logics as
used in our work are limited to binary predicates), it is based on the closed
world assumption (if something cannot be inferred to be true, it is inferred to
be false) whereas we rely on the open world assumption, which is more realistic
in real world scenarios. A probabilistic extension of \textsc{KnowRob}, called
\textsc{ProbCog}~\cite{Jain2009} is also available. While in principle
possible, currently the CRAM architecture does not provide explicit support for
interacting with humans.

\subsection*{Related work}

\fxfatal{Merge the section with previous one}

The human presence brings new requirements for robot's abilities both
at the functional and at the deliberative levels~\cite{Klein2004}. The
topics involve motion~\cite{Kulic2007,Berg2004,Madhav2006},
navigation~\cite{Althaus2004,Sisbot2007}, manipulation~\cite{Kemp2007}
in presence of humans as well as perception of human
activities~\cite{Breazeal2001,Burger2008}. Also, when
interacting with humans, robots need to incorporate communication and
collaboration abilities. Several theories dealing with
collaboration~\cite{Cohen1991,Grosz1996,Clark1996} emphasize that
collaborative tasks have specific requirements compared to individual
ones, \eg, since the robot and the person share a common goal, they
have to agree on the manner to realize it, they must show their
commitment to the goal during execution, etc. Several robotic systems
have already been built based on these
theories~\cite{Rich1997,Sidner2005,Tambe2005a,Breazeal2003} and they
all have shown benefits of this approach. They have also shown how
difficult it is to manage turn-taking between communication partners
and to interleave task realization and communication in a generic
way. Finally, today only few
systems~\cite{Fong_2006,Breazeal2003,Sisbot2008} take humans into
account at all levels.

Perspective Taking is a human ability which allows one to put
him/herself in another person's point of view. Studied in
psychology literature~\cite{Flavell1992,Tversky1999}, this ability is
crucial when interacting with people by allowing one to reason on
others' understanding of the world in terms of visual perception, spatial
descriptions, affordances and beliefs, etc.
Therefore, in the last years these notions have been gradually
employed in Human-Robot Interaction.~\cite{breazeal2006} presents a
learning algorithm that takes into account information about a
teacher's visual perspective in order to learn a
task. ~\cite{Johnson2005} apply visual perspective taking for action
recognition between two robots.~\cite{Trafton2005} use both visual and
spatial perspective taking for finding out the referent indicated by a
human partner.

Spatial reasoning~\cite{OKeefe1999}, on the other hand, has been used
for natural language processing for applications such as direction
recognition ~\cite{Kollar10,Matuszek10} or language
grounding~\cite{Tellex10}.~\cite{Skubic2004} presented a spatial
reasoner integrated in a robot which computes symbolic positions of
objects.


\subsection{Chapter overview}

The introduction  of this chapter already presented the overall approach for
grounding, reasoning and making decision in human-robot interaction context.

Section~\ref{spark} presents {\sc SPARK} (for \emph{SPAtial Reasoning \& Knowledge}), the
component that merges perceptual information with a coherent geometric model and
builds a symbolic interpretation of the world from the robot's point of view,
as well as an individual symbolic model for each agent currently present in the
environment. 

The next section we present the {\sc ORO}
(\emph{OpenRobots Ontology} server) knowledge base. We present it first, along with its objectives,
since it is the knowledge \textit{hub} of the system, used pervasively by other
components.

We then present two advanced applications:

\begin{inparaenum}

\item \emph{Symbolic task planing} (section~\ref{sec:plan}): ...\fxfatal{to do!}

\item \emph{Natural language processing} (section~\ref{dialog}): the third component includes natural
language parsing capabilities, interactive disambiguation routines and concept
anchoring. We focused our efforts on three classes of utterances commonly found
in human-robot interaction: \emph{statements} (\ie new facts the human wants to
inform the robot), \emph{orders} (or more generically \emph{desires}) and
\emph{questions on declarative knowledge} (whose answers do not require
planning or prediction)\footnote{This would roughly cover the \emph{representative}
(sometimes referred as \emph{assertive}) and \emph{directive} illocutionary
acts in Searle's~\cite{Searle1976} classification.}.

\end{inparaenum}




Finally, Section~\ref{conclusion} concludes and discusses the work presented 
in this paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Building a Symbolic Model of the Environment}
\label{sec:situ}

Building, updating and maintaining a correct state of the world at
geometric and symbolic level is crucial to the capacity of the robot
to carry on successfully a multi-step interaction with a human. Tight
integration between the robot controller and the geometric and
temporal reasoning functions in SPARK and symbolic facts and beliefs
management in ORO is central.


Anchoring perceptions in a symbolic model requires perception abilities
and their symbolic interpretation. In this section we present
SPARK (\emph{SPAtial Reasoning \& Knowledge}~\cite{Sisbot2011}), a situation assessment reasoner
that generates relevant symbolic information from the geometry of the
environment with respect to relations between objects, robots and humans.
Moreover, the notion of \emph{Perspective
Taking}~\cite{Flavell1992,Tversky1999} is employed at the heart of the
reasoner to provide the robot with the ability to put itself at the human's
place and to reason about the world from different perspectives.


\subsection{Locations}

One way of referring to object's positions is based on human's symbolic 
descriptors, instead of using their precise position. In fact, in many cases, 
this information is the most precise information available since humans do not 
store the numeric coordinates of objects. These type of descriptors have been 
studied in the context of language grounding 
(\cite{O'Keefe1999,Matuszek2010,Regier2001,Kelleher2009,Blisard2005}). In this 
work we focus on the following relations which are computed with respect to the 
position of the agents and the objects:

\begin{itemize} 

\item \emph{Location according to an agent}: The predicate
\concept{isLocatedAt} represents spatial locations between agents and objects.
For example we say ``it is on my right, on your left, ...'' We compute these
spatial locations by dividing the space around the referent (an agent) into $n$
regions based on arbitrary angle values relative to the referent orientation.
For example, for $n = 4$ we would have the space divided into \emph{front,
left, right} and \emph{back}. Additionally, two proximity values, \emph{near}
and \emph{far}, may also be considered. The number of regions and proximity
values can be chosen depending on the context where the interaction takes
place.

\item \emph{Location according to an object}: We can also refer to object
locations with respect to other objects in the environment, such as \emph{above,
next to, in}, etc. In this work we compute three main relations based on the
bounding box and center of mass of the objects 
(Figure~\ref{fig::sprelations}\emph{c}): 

\begin{itemize}
	\item \concept{isOn}: computes if an object $O_1$ is on another object $O_2$ by
	evaluating the center of mass of $O_1$ according to the bounding box of $O_2$.

	\item \concept{isIn}: evaluates if an object $O_1$ is inside another object
	$O_2$ based on their bounding boxes $BB_{O_1}$ and $BB_{O_2}$.

	\item \concept{isNextTo}: indicates whether an object $O_1$ is next to another
	object $O_2$. We cannot use a simple distance threshold to determine if two
	objects are next to each other since the relation is highly dependent on the
	dimensions of the objects. For instance, the maximum distance between large
	objects (\eg two houses) to consider them as being next to each other is much
	larger than the maximum distance we would consider for two small objects (\eg
	two bottles). Thus, the relation between the dimensions and the distances of
	the objects are taken into account.  

\begin{figure} 
	\centering
	\includegraphics[width=0.6\columnwidth]{figs/spatial_relation.png}
	\caption{Spatial relations between two objects: A) \concept{isOn} relation, 
	B) \concept{isIn} relation, and C) \concept{isNextTo} relation.} 
	\label{fig::sprelation} 
\end{figure}

\end{itemize} 
\end{itemize}

To ensure the different agent models are up-to-date, all these properties are
always computed on-line, each time the current state of the world changes.

SPARK can be compared to the \emph{Grounded Situation Model} (GSM) introduced
by Mavridis and Roy~\cite{Mavridis2005} in the sense that they both provide an
amodal physical representation of the world used as a mediator between the
sensor space and symbolic models. They have however different features: while
GSM enables representation of time and imaginary objects (whose existence is
hinted by verbal assertions from a human, also called \emph{presupposition
accomodation}), SPARK offers a richer 3D model that enables the computation of
several spatial relationships between objects and an effective implementation
of perspective taking capabilities.

Table~\ref{facts|sprelations} lists all the symbolic relationships that are
currently computed by the system.

\begin{table}[h]
    \centering
    \begin{tabular}{p{2cm}p{5cm}p{2cm}l}
	\rowcolor{white}
    \textbf{Subject} & \textbf{Predicate} & \textbf{Object} & \emph{Details} \\ 
    \hline
	 a location \par (\concept{Location})  & \textbf{is at} \par (\concept{isAt} $\equiv$ \concept{cyc:objectFoundInLocation})  &  a location \par (\concept{Location}) & \\ 
	 &  $\rightarrow$ \textbf{is on} \par (\concept{isOn} $\equiv$ \concept{cyc:above\_Touching})  &  & \\ 
	 &  $\rightarrow$ \textbf{is in} \par (\concept{isIn})  &  & \\ 
	 &  $\rightarrow$ \textbf{is next to} \par (\concept{isNextTo})  & &  \\ 
	 a location \par (\concept{Location})  & \textbf{is above} \par (\concept{isAbove} $\equiv$ \concept{cyc:above-Generally})  &  a location \par (\concept{Location})  &  inverse of \concept{isBelow} \\ 
	 a location \par (\concept{Location})  & \textbf{is under} \par (\concept{isBelow})  &  a location \par (\concept{Location})  &  inverse of \concept{isAbove}
	\end{tabular}
   \caption{List of facts describing spatial relationships between objects.}
   \label{facts|sprelations}
\end{table}

Anonymous locations are represented by the mean of new randomly generated
location IDs. 

For instance, location of \concept{BottleA} has no name $\rightarrow$ \stmt{bottleA
isAt loc12578}, with \concept{loc12578} a random ID. 

If the position is actually completely unknown, \concept{loc12578} has no
further informations attached to it (and can be omitted altogether). 

If the position is known but has no name, it is possible to add details to the
location. For instance: \stmt{loc12578 owl:differentFrom MyNiceTable}. 



\subsubsection{Agent independent world dynamics}
\begin{table}[h]
   \centering
   \begin{tabular}{p{2cm}p{5cm}p{2cm}l}
	\rowcolor{white}

	\textbf{Subject} & \textbf{Predicate} & \textbf{Object} \\ 
	\hline
	 a spatial enduring thing \par (\concept{EnduringThing-Localized})  & \textbf{is moving} \par (\concept{isMoving})  &  a boolean \par (\concept{xsd:boolean})  
	\end{tabular}
   \caption{List of facts describing agent independent world dynamics.}
\end{table}


\subsection{Exploration and search policies}

The robot controller has access to the symbolic facts in ORO that are
automatically updated whenever object and agent positions are changed.
Robot controller can also access geometric perceived or inferred
positions of objects and geometric positions and postures of the human
that will be used to orient its cameras.  Building and updating the
state of the world first relies on perceiving objects. Robot
controller can use:

\begin {itemize}
\item Exploration policies: robot will exhaustively scan the table to
  see all what can be seen.

\item Search policies: robot will search an object until it is
  detected if possible, scanning all the table and looking in human
  hand.

\end {itemize}

Robot reasons on possible positions for non perceived
objects. These hypotheses could be updated using new input from dialogue, human
action and focus of attention. Currently, we manage at most one hypothesis per
object. This hypothesis is produced by robot controller through an inference on
robot or human action. In case of perception conflicts with low probability for
the current hypothesis, robot controller will break this hypothesis and delete
corresponding symbolic fact in the ontology.

Robot must reacts to change in the world not linked to robot
action to drive world update. Robot controller uses SPARK to monitor human hand
motion and primitive action recognition for \emph{pick} and \emph{throw}. As
mentioned above these primitive action recognition should be used with higher
level information on goals, intentions and plans and some exploration to
achieve complex human action and activity monitoring.  In the current
implementation, these primitive actions are over-optimistically interpreted as
the corresponding actions \emph{pick} object and \emph{throw} object.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Building a Symbolic Representation of Agents}
\label{grounding_agents}

\fxfatal{Introduction: 2 aspects: situation assessment -> current agent capabilities,
planning assessment -> ''mightabilities''}

\subsection{Agent Capabilities}

There are a number of common properties for a robot and a human related to
their capabilities in a given situation: they can both reach, grasp, look at,
point at, etc. In our context, we group robots and humans into a single category.
Thus, we define agents as entities that can act in the environment and
manipulate it. In this work we focus on the following capabilities from each
agent's perspective\footnote{Note that each of the capabilities described are
computed from each agent point of view, and therefore, also stored in different
models in ORO for further use at the decisional level.}:

\begin{itemize}

\item \emph{Sees}: An important ability to know about an agent is to predict
``what it can see'', \ie what is within its field of view (FOV). A robot being
able to compute this information can then act accordingly. An example
would be a clarification scenario where the human is searching for an object
and the robot is able to infer that he/she is looking for the one that is not
visible (otherwise the user would not be searching for it).
In Figure~\ref{fig::sparkRepresentations}\emph{a} the field of view of a person is illustrated with a
grey cone (broader one). While he is able to see the two small boxes on the
table in front of him, the big box on his right is out of his FOV, and
therefore, he is not able to see it. 

\item \emph{Looks At}: this relation corresponds to what the agent is focused
on, \ie where its focus of attention is directed. This model is based on a
narrower field of view, the field of attention (FOA). 
Figure~\ref{fig::sparkRepresentations}\emph{a}
shows the field of attention of a person with a green cone (narrower one). In
this example only the grey box satisfies the \concept{looksAt} relation.

\item \emph{Points At}: verifies whether an object is pointed at by an agent.
This relation is particularly useful during interaction when one of the agents
is referring to an object saying ``this" or ``that" while pointing at it.
Section~\ref{disamb-gestures} describes in more detail the combination of both
sources of information (verbal and non-verbal).
 
If a big object occludes a smaller one, and an agent is pointing at them, the
outcome of the evaluation will result only in one relation, \ie \stmt{agent\_01
pointsAt object\_01} since the small one is not visible to the agent.  On the
contrary, if the small object is in front of the big one, then both objects
will satisfy the relation, which may generate an ambiguity (which object the
agent refers to?) that should be solved through higher level reasoning (\eg
context analysis or clarification through verbal interaction).

\item \emph{Reachable}: it allows the robot to estimate the agent's capability
to reach an object, which is fundamental for task planning. For example, if the
user asks the robot to give him/her an object, the robot must compute a transfer
point where the user is able to get the object afterward. 
Figure~\ref{fig::sparkRepresentations}\emph{b} shows different reachability postures for each object
on the table. In the example, the bottle and the box are both reachable for the
human, but the teddy bear is too far. Instead, from the robot's perspective,
the teddy bear is reachable for it, while the bottle is not.

\end{itemize}

\begin{figure*}[!t]
	\begin{center}
	\subfigure[]{
		\includegraphics[width=0.35\linewidth]{figs/looks.jpg} 
		\label{(a)}
	}
	\subfigure[]{
		\includegraphics[width=0.3\linewidth]{figs/reach.jpg}
		\label{(b)}
	} 
	\caption{(a) Field of view (FOV) and the field of attention (FOA) of the human. (b) Different reaching postures for the human.}
	\label{fig::sparkRepresentations}
	\end{center}
\end{figure*} 


While the first three relations (\concept{sees}, \concept{looksAt} and
\concept{pointsAt}) are computed through a model based approach, the latter one
is based on the Generalized Inverse Kinematics with pseudo inverse
method~\cite{Nakamura90,Baerlocher04} to find a collision free posture for the
agent where its end-effector is at the center of the object within a given
tolerance. The details of these computations are out of the scope of this
article.

Figure~\ref{fig::reach-ex} illustrates different situations for the
\textit{reachable} relation. In this case, the robot and its human partner
are placed face to face, in a table-top setup (Figure~\ref{fig::reach-ex}.1).
The robot first estimates if the small grey box is reachable to itself. This is
done by finding a collision free posture to reach the object
(Figure~\ref{fig::reach-ex}.2). Next the robot switches to the human's
perspective to estimate if the same object is reachable to the human as well.
In the last scene, the human moves towards his left, farther from the object
(Figure~\ref{fig::reach-ex}.4). The situation is then reevaluated. In this
occasion though, the reasoner cannot find a satisfactory posture for the human
to reach the box because he is too far from the target.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=1.0\textwidth]{figs/reachex.jpg}	
	\caption{An example illustrating the \textit{reachable} relation. The relation is computed from the perspectives of both the robot and the human. The computed posture at each step is illustrated with a global view of the scene (top), and from a closest view (bottom).}
\label{fig::reach-ex}
\end{figure*}

\begin{figure}[ht!]
   \label{fig:sparkSubfigures}
   \begin{center}
%
       \subfigure[Initial state]{%
%           \label{fig:wuweiPhoto}
           \includegraphics[width=0.5\textwidth]{./figs/etat2-P1010769_brightened-v2.jpg}
       }%
       \subfigure[3d model view of initial state]{%
%          \label{fig:sparkScreenshot}
          \includegraphics[width=0.43\textwidth]{./figs/etat2_photo.png}
       }\\ %  ------- End of the first row ----------------------%
%
   \end{center}

   \caption{%
     In this situation, there are three tapes on the table. Two tapes
     are only reachable by the robot: the LOTR\_TAPE (black in the 3d
     model) and GREY\_TAPE. The third tape WALLE\_TAPE (white in the
     3d model) and the trashbin PINK\_TRASHBIN are only reachable by
     the human HUMAN1. All tapes are on the table TABLE.  }%

\end{figure}

The set of facts computed in the situation depicted by
Figure~\ref{fig:sparkSubfigures} is the following:
\begin{footnotesize}
%\begin{small}
\begin{verbatim}
                ROBOT                          HUMAN1
PINK_TRASHBIN isReachable false    PINK_TRASHBIN isReachable true 
WALLE_TAPE isReachable false       WALLE_TAPE isVisible true 
LOTR_TAPE isReachable true         LOTR_TAPE isReachable false 
GREY_TAPE isReachable true         GREY_TAPE isReachable false
WALLE_TAPE isVisible true          WALLE_TAPE isReachable true 
LOTR_TAPE isVisible true           LOTR_TAPE isVisible true 
GREY_TAPE isVisible true           GREY_TAPE isVisible true 
WALLE_TAPE isOn TABLE              WALLE_TAPE isOn TABLE 
LOTR_TAPE isOn TABLE               LOTR_TAPE isOn TABLE 
GREY_TAPE isOn TABLE               GREY_TAPE isOn TABLE 
\end{verbatim}
%\end{small}
\end{footnotesize}


The following tables provide the exhaustive list of agent-related symbolic facts
currently produced by the system.

\begin{table}[h]
	\centering
	    \begin{tabular}{p{2cm}p{5cm}p{2cm}l}
		\rowcolor{white}
		\textbf{Subject} & \textbf{Predicate} & \textbf{Object} & \emph{Details} \\
		\hline
	 a location	\par (\concept{Location})  & \textbf{has relative position}	\par (\concept{hasRelativePosition})  &  a location	\par (\concept{Location}) & \\ 
	 & 	$\rightarrow$ \textbf{is behind} \par (\concept{behind} $\equiv$ \concept{cyc:behind-Generally})  &  & inverse of \concept{inFrontOf}  \\ 
	 &  $\rightarrow$ \textbf{is in front of} \par (\concept{inFrontOf} $\equiv$ \concept{cyc:inFrontOf-Generally})  & 	 & 	 inverse of \concept{behind}  \\ 
	 &  $\rightarrow$ \textbf{is left of} \par (\concept{leftOf})  &  &  inverse of \concept{rightOf} \\ 
	 &  $\rightarrow$ \textbf{is right of} \par (\concept{rightOf})  & 	 & 	 inverse of \concept{leftOf}  \\ 
	 a object \par (\concept{Object})  & \textbf{is far from}	\par (\concept{cyc:farFrom})  &  a agent \par (\concept{Agent}) & \\ 
	 a object \par (\concept{Object})  & \textbf{is near of} \par (\concept{cyc:near})  &  a agent \par (\concept{Agent}) & 
	\end{tabular}
	\caption{List of facts describing relative spatial relationships between objects and agents.}
\end{table}


\begin{table}[h]
	\centering
		\begin{tabular}{p{3cm}p{5cm}p{3cm}p{3cm}}
		\rowcolor{white}
		\textbf{Subject} & \textbf{Predicate} & \textbf{Object} & \emph{Details} \\
		\hline
		 a agent \par (\concept{Agent})  & \textbf{looks at} \par (\concept{looksAt})  & a spatial enduring thing \par (\concept{EnduringThing-Localized}) \\
		 a agent \par (\concept{Agent})  & \textbf{sees} \par (\concept{sees})  &  a spatial enduring thing \par (\concept{EnduringThing-Localized})  &    \\ 
		 a spatial enduring thing \par (\concept{EnduringThing-Localized})  & \textbf{is in field of view} \par (\concept{isInFieldOfView})  &  a boolean \par (\concept{xsd:boolean})  & via inference: \par \stmt{myself sees *} $\Leftrightarrow$ \stmt{* isInFieldOfView true} \\ 
		 a agent \par (\concept{Agent})  & \textbf{points at} \par (\concept{pointsAt} $\equiv$ \concept{cyc:pointingToward})  &  a spatial enduring thing
\par (\concept{EnduringThing-Localized}) \\ 
		a agent \par (\concept{Agent})  & \textbf{focuses on} \par (\concept{focusesOn})  &  a spatial enduring thing \par (\concept{EnduringThing-Localized})  &  via inference: \par \concept{looksAt} $\wedge$ \concept{pointsAt} $\Rightarrow$ \concept{focusesOn}

	\end{tabular}
	\caption{List of facts describing the attentional state of an agent. \concept{looksAt} is interpreted as an object \emph{being in the field of attention} of an agent. An object is \concept{see}n if it is visible for the agent without moving the head (ie, in \emph{field of view}).}
\end{table}


\begin{table}[h]
	\centering
		\begin{tabular}{p{2cm}p{5cm}p{2cm}l}
		\rowcolor{white}
		\textbf{Subject} & \textbf{Predicate} & \textbf{Object} & \emph{Details} \\
		\hline
		a spatial enduring thing \par (\concept{EnduringThing-Localized})  & \textbf{is visible (with possibly a head movement)} \par (\concept{isVisibleByMovingHead})  &  a boolean \par (\concept{xsd:boolean})  \\ 
		a agent \par (\concept{Agent})  & \textbf{sees (with possible a head movement)} \par (\concept{seesWithHeadMovement})  &  a object \par (\concept{Object})   & via inference: \par \stmt{myself seesWithHeadMovement *} $\Leftrightarrow$ \stmt{* isVisibleByMovingHead true}) \\
		a agent \par (\concept{Agent})  & \textbf{can not see (with possible a head movement)} \par (\concept{cannotSeeWithHeadMovement})  & a object \par (\concept{Object})   &  via inference: \par \stmt{myself cannotSeeWithHeadMovement *} $\Leftrightarrow$ \stmt{* isVisibleByMovingHead false}) \\ 
		a object \par (\concept{Object})  & \textbf{is reachable} \par (\concept{isReachable})  &  a boolean \par (\concept{xsd:boolean})  \\ 
		a agent \par (\concept{Agent})  & \textbf{reaches} \par (\concept{reaches})  &  a object \par (\concept{Object})   & via inference: \par \stmt{myself reaches *} $\Leftrightarrow$ \stmt{* isReachable true}) \\ 
		a agent \par (\concept{Agent})  & \textbf{can not reach} \par (\concept{cannotReach})  & a object \par (\concept{Object})   & via inference: \par \stmt{myself cannotReach *} $\Leftrightarrow$ \stmt{* isReachable false})

	\end{tabular}
	\caption{List of facts describing action potentialities for an agent.}
\end{table}


\begin{table}[h]
	\centering
	\begin{tabular}{p{2cm}p{5cm}p{2cm}}
		\textbf{Subject} & \textbf{Predicate} & \textbf{Object} \\
		\hline
		a agent \par (\concept{Agent})  & \textbf{has in left/right hand} \par (\concept{hasIn\{Left|Right\}Hand})  &  a GraspableObject \par (\concept{GraspableObject}) \\ 
		a agent \par (\concept{Agent})  & \textbf{has posture} \par (\concept{hasPosture})  &  a posture \par (\concept{Posture})  \\
		a agent \par (\concept{Agent})  & \textbf{currently 'bodily' does} \par (\concept{currentlyBodilyDoes})  &  an action \par (\concept{Action})
	\end{tabular}
	\caption{List of facts describing the state of an agent in general.}
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{p{2cm}p{5cm}p{2cm}}
		\textbf{Subject} & \textbf{Predicate} & \textbf{Object} \\
		\hline
		a agent \par (\concept{Agent})  & \textbf{currently performs} \par (\concept{currentlyPerforms})  &  a action \par (\concept{Action})
	\end{tabular}
	\caption{List of facts describing the current state of a robot.}
\end{table}

\subsection{Hypotheses on objects states and positions}

It is sometimes difficult or even impossible to see and/or track an object in
certain states. This happens, for instance, when the object has been put in a
container, when it is in the robot gripper or in the human hand, and more
generally in any state in which it is hidden by something else. Our robot has a
model of the possible symbolic states for an object (whether the object is on a
furniture, in an agent hand, in a container, etc.).  According to the robot
perception of what has happened since the object was last seen, the robot tries
to maintain a belief of the current possible symbolic states and their
associated probabilities for this object. Such information can be used to
update the beliefs using input from exploration, dialog, human visual focus,
etc.

SPARK currently provides a simple implementation of such a functionality. The
only managed hypotheses are \emph{in container} and \emph{in agent hand}. We
can have only one hypothesis at the same time. Hypothesis validity is checked
geometrically in case of incoming perception values.


\subsection{Primitive action recognition}

Monitoring human activity is crucial to maintain a coherent state of the world.
Full human action and activity monitoring is a difficult task that requires
knowledge and reasoning both on high level facts like goals, intentions and
plans, as well as bottom-up data from agent and object motions. Simple temporal
and geometric reasoning on human hand trajectories and potential objects
placements can provide some useful clues for high level human monitoring
processes. We call this temporal and geometric reasoning \emph{primitive action
recognition}.

For example, a \emph{pick}, a \emph{throw} or a \emph{place} action can be
recognized by observing that an object on table and an empty human hand are
close to each other, or that the human hand holding an object is close to a
container, etc. Human hand position is either directly perceived or inferred
from its initial perceived trajectory.  We have a simple implementation of such
a primitive action recognition in SPARK that relies on monitoring human hand
and its motion near objects or above containers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Mightabilities: pro-active analysis of potentialities}

\fxfatal{Write an introduction that glues this section with the previous ones}

In human movement and behavioral psychology literatures, different types of
reach actions of the human have been identified and analyzed,
\cite{Gardner2001, Choi2004}.  Figure~\ref{fig|reaches_taxonomy} shows taxonomy
of such reaches involving simple arm-shoulder extension (arm-and-shoulder
reach), leaning forward (arm-and-torso reach) and standing reach.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{./figs/reach_postures.pdf} \\
  \caption {Taxonomy of reach actions:(a) arm-shoulder reach, (b) arm-torso 
  reach, (c) standing reach.}
  \label{fig|reaches_taxonomy}
\end{figure}

Taking inspiration from such studies, we have presented the concept of
\emph{Mightability}, which stands for ``Might be Able to...'', as a framework for
\emph{multi-state visuo-spatial} perspective taking. The idea is to analyze the
various abilities of an agent such as ability to see or ability to reach, not
only from the current state of the agent, but also from a set of states, which
the agent might achieve from his current state. For this the robot applies\fxwarning{Sentence unclear}, $A_v$,
an ordered list of virtual actions, to make the agent to virtually reach a state
and then estimates the abilities $A_b \in \{See, Reach, Grasp\}$, by respecting the
environmental and postural constraints of the agent. Currently,

\[ 
A_v \subseteq \{A_v^{head}, A_v^{arm}, A_v^{torso}, A_v^{posture}, A_v^{displace}\}
\]

where

\begin{align*}
A_v^{head} & \subseteq \{PanHead, TiltHead\} \\
A_v^{arm} & \subseteq \{StretchOutArm\} \\
A_v^{torso} & \subseteq \{TurnTorso, LeanTorso\} \\
A_v^{posture} & \subseteq \{MakeStanding, MakeSitting\} \\
A_v^{displace} & \subseteq \{MoveTo\} \\
\end{align*}

When such mightability analyses are performed at the levels of cells of the
discretized 3D workspace, we term it as \emph{Mightability Maps} (MM) and when done
for the object in the space we call it \emph{Object Oriented Mightabilities} (OOM).

The robot performs mightability analyses by taking into account collision
as well as the joint limits. The information about the robot and human
positions, orientations and the 3D state of the environment is continuously
updated in our 3D representation and planning platform, Move3D \cite{Simeon2001}, which
also facilitates our robots to check intra-body and inter-body collisions
for itself and for the human model. The robot uses kinematic structures of
the agents and performs various virtual actions until the joint limits
of the neck and/or torso are reached or the collision of the torso of
the agent with the environment is detected.

\begin{figure}[ht!]
   \begin{center}
%
       \subfigure[]{%
           \label{a}
           \includegraphics[width=0.5\textwidth]{./figs/mightabilities/2a.jpg}
       }%
       \subfigure[]{%
          \label{b}
          \includegraphics[width=0.5\textwidth]{./figs/mightabilities/2b.jpg}
       }\\ %  ------- End of the first row ----------------------%
       \subfigure[]{%
           \label{c}
           \includegraphics[width=0.5\textwidth]{./figs/mightabilities/2c.jpg}
       }%
       \subfigure[]{%
          \label{d}
          \includegraphics[width=0.5\textwidth]{./figs/mightabilities/2d.jpg}
       }\\
       \subfigure[]{%
          \label{e}
          \includegraphics[width=0.5\textwidth]{./figs/mightabilities/2e.jpg}
       }   
	\end{center}

   \caption{%
		(a) 3D representation of the current world state. Mightability Maps for
		human (b) to see and (c) to reach the places in 3D from his current
		position and configuration. Because of robot's capability to perform
		multi-state perspective taking, the robot is able to estimates that if
		human will turn his head around, he can see more places (d) and if he
		will lean forward he can reach more places (e).
   }
		
   \label{fig|mightabilities}
\end{figure}

Figure~\ref{fig|mightabilities}(a) shows the current scenario.
Figures~\ref{fig|mightabilities}(b) and (c) show the MM for the places visible
and reachable by the human from his current position and configuration.

Thanks to multi-state perspective taking, the robot is also able to estimate
that if the human will just turn his head around, he will be able to see more
places (figure~\ref{fig|mightabilities}(d)) and if the human will lean forward,
he will be able to reach more space (figure~\ref{fig|mightabilities}(e)). For
reachability, the robot further distinguishes among reachable by left hand,
reachable by right hand and reachable by both hands of an agent in a particular
state.  

Mightability analysis could be performed and updated online; hence facilitate
its use for various planning and decision making components as explained below.
Mightability Map based finding candidate places for basic HRI
tasks\fxwarning{Sentence not clear}: As the robot is able to perform such
analysis for all the agents in the environment, it can use the MM to find the
candidate places to perform a set of basic HRI tasks: give, make accessible,
show, hide an object, etc. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{./figs/mightab-steps.pdf}

\caption { Mightability Analysis-based framework for finding a solution: (a)
Initial Mightability Maps, (b) Decision making on relevant Mightability Maps
depending on task and required comfort level of agents, (c) relevant
Mightability Maps, (d) task specific set operations, (e) raw candidate solution
set, (f) weight assignment based on spatial preferences, (g) set of weighted
candidate points, (h) applying rigorous and expensive tests on reduced search
space, (i) the feasible solution of highest weight.}

  \label{fig|mightabilities-framework}
\end{figure}


As shown in Figure~\ref{fig|mightabilities-framework}, for
finding a feasible solution of a task, robot reasons on various MM of
the involved agents and performs relevant set operations depending upon
the task.  

\begin{figure}[ht!]
   \begin{center}
       \subfigure[]{%
           \label{a}
           \includegraphics[width=0.5\textwidth]{./figs/mightabilities/4a.jpg}
       }%
       \subfigure[]{%
          \label{b}
          \includegraphics[width=0.5\textwidth]{./figs/mightabilities/4b.jpg}
       }\\ %  ------- End of the first row ----------------------%
       \subfigure[]{%
           \label{c}
           \includegraphics[width=0.5\textwidth]{./figs/mightabilities/4c.jpg}
       }%
       \subfigure[]{%
          \label{d}
          \includegraphics[width=0.5\textwidth]{./figs/mightabilities/4d.jpg}
       }
	\end{center}

   \caption{
	Steps for finding weighted candidate search space for the task of robot
	giving an object to the human.}
		
   \label{fig|mightabilities-steps}
\end{figure}



Figure~\ref{fig|mightabilities-steps} shows the main steps of finding the
candidate search space for the task of giving an object to the human by the
robot. From the initial set of all the MM for the robot and for the human, the
planner extracts the relevant MM based on the task and the desired efforts of
the agents.

In this example, the maximum desired effort for the human is to lean forward.
As the task requires a hand-over operation so the relevant MM obtained in step
(c) corresponds to the reach and visibility of both the agents, as shown in
figures~\ref{fig|mightabilities-steps}(a) and (b) for the robot and for the
human respectively.

Then the planner performs the set operations in step (d) to obtain the raw
candidate points in step (e). For the example task, the set operation is
finding the intersection of reachable and visible places by both the agents.
Figure~\ref{fig|mightabilities-steps}(c) shows the resultant candidate points
obtained in step (e), which in fact is commonly reachable and visible by both
the agents. 

Further based on various criteria such as comfort, preferences, etc. weights
are assigned to the raw candidate points in step (f) to obtain weighted
candidate points in step (g). Figure~\ref{fig|mightabilities-steps}(d) shows
the weighted candidate points, red cells are least preferable and green are
more preferable, so highest weight. At this step it is interesting to note that
the search space has been significantly reduced as compared to entire
workspace.  

In step (h) each candidate cell is iteratively tested for feasibility in the
order of highest to lowest weight until a solution is found. For finding a
feasible solution, various task-dependent constraints are introduced. Such
tests would have been very expensive if done for entire workspace.  In our
current implementation the first highest weighted candidate cell which
satisfies all the feasibility tests is accepted as the feasible solution of
highest weight, as shown in step (i). 

%Figure~\ref{fig|pr2-giving} show PR2 robot performing the task of giving an
%object to the human by finding the feasible solution through the presented
%framework. 
%
%
%\begin{figure}
%  \centering
%  \includegraphics[width=0.8\textwidth]{./figs/mightabilities/5.png}
%  \caption { 
%PR2 giving an object to the human at a feasible place, which is easily visible
%and reachable by the human from his current position.
%}
%  \label{fig|pr2-giving}
%\end{figure}
%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Explicit Knowledge Management}
\label{cognitivekernel}

\subsection{ORO: The OpenRobots Ontology server}

A key element of this cognitive architecture is how knowledge is explicitely
exposed, stored and handled between component.

We have adopted a centralized approach for knowledge management (while this is
the most commonly found approach in the litterature, other, more distributed
approaches exist, like \cite{Jacobsson2008}). Our knowledge base is called
ORO~\cite{Lemaignan2010}.

The ORO platform is primarily designed as a central
knowledge storage service implemented as a server where the robot
components can add or query statements at run-time. Figure~\ref{fig|oro-overview}
illustrates the overall architecture. The \emph{front-end} accepts and manages
connections from client components. The clients' requests are processed by a
set of internal modules: basic operations on statements, but also higher
cognitive and human-robot interaction related functionalities are available
(detailed thereafter). External plugins can also be easily added. The modules
rely on several parallel ontology \emph{back-ends} where the knowledge is
actually stored.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.8\linewidth]{figs/oro_architecture_functional.pdf}
  \caption{Overview of the ORO architecture.}
  \label{fig|oro-overview}
\end{figure}

Besides acting as a facts database, the ORO platform exposes several functions:
operations on knowledge statements relying on inference (through a continuous
first-order logic classification process), management of \emph{per-agent}
symbolic models, and also higher cognitive and human-robot interaction related
functionalities like categorization of sets of concepts, profiles of memory
(that enable the robot to ``forget'' about some facts), natural language
grounding~\cite{Lemaignan2011}\ldots.

ORO stores independent knowledge models (in our implementation, as
\emph{ontologies}) for each agent (the robot and the humans it
interacts with). The robot architecture components (like the executive
layer or the situation assessment component) can then store the
agents' beliefs in specific models.  Each of these models is
independent and logically consistent, enabling reasoning on different
perspectives of the world that would otherwise be considered as
globally inconsistent (for instance, an object can be visible for the
robot but not for the human. This object can have at the same time the
property \concept{isVisible \textbf{true}} and \concept{isVisible
  \textbf{false}} in two different models). This feature actually
allows us to consider the robot to be endowed with a simple
\emph{theory of mind}~\cite{Scassellati2002}: it can explicitly
model the belief state of its interactors.

ORO also provides an event mechanism that allows components to be
triggered when specific events occur. A component can
for instance subscribe to events of kind \setstmt{?agent isVisible
  true, ?agent type Human}. As soon as the perception layer detects a
human in the robot's field of view and accordingly updates the
knowledge base, the executive layer would be triggered back. The
event framework also takes advantage of the inference capabilities of
ORO. Thus an event can be indirectly triggered if its triggering
conditions can be inferred to be true.

\subsection{The OpenRobots Common-Sense Ontology}
\label{ontology}

For the robot to interpret and come up with new inferences based on the facts
coming from the perceptual layers, it need a \emph{cultural background}, a
common-sense knowledge assumed to be shared by all agents. The ORO server is
loaded at startup with an initial set of statements which we call the
\emph{OpenRobots Common Sense Ontology}. It defines a small set of concepts
(and implicitly, a vocabulary) that can be used by all the modules of the robot
to unambiguously add or query facts. Moreover, the same ontology declares rules
and logical properties that are later on used for inference.

The \emph{OpenRobots Common Sense Ontology} defines a small set of classes (56
are currently defined) and predicates (60 are currently defined) focused on
concepts useful for human-robot interaction. It includes both very broad
categories like \concept{SpatialThing}, \concept{Event} or \concept{Action},
and much more concrete concepts as \concept{Table}, \concept{Book} or colors.
Available predicates allow us to describe the state of the agents and the world
with relations like \concept{isOn}, \concept{sees},
\concept{currentlyPerforms}, etc.

Several significant projects are trying to provide such a machine-processable
repository of common sense facts produced by humans (the \textsc{OpenMind}
project\footnote{\url{http://www.openmind.org/}}, for instance). These
knowledge bases are valuable but remain difficult to use in a pervasive way
because of both their incompleteness and the lack of good connections with
underlying, unambiguous concepts.
%\cite{Singh2002}

Our common sense ontology is closely aligned with the open-source
OpenCyc\footnote{\url{http://www.opencyc.org}} upper ontology.
OpenCyc defines a large taxonomy of concepts and semantic
relationships between concepts that are used in several other projects
(\textsc{WordNet, DBpedia}). This potentially eases the exchange and addition
of knowledge from these other sources. Moreover, it also enables knowledge
exchange with other robots (for instance, the works previously mentioned by
Daoutis and Tenorth rely on the same Cyc concepts).

\subsection{Reasoning and Dynamic Knowledge Structuring}

As previously mentioned, ontologies in ORO are written in OWL. The
Pellet reasoner supports most of the OWL constructs and allows several types of
reasoning:

\begin{itemize}
	\item inheritance
	\item property axioms
		\begin{itemize}
		\item entailments based on predicates' domain and range,
		\item cardinality constraints (including \concept{allValue}, 
		\concept{someValue}, \concept{hasValue}),
		\item property characteristics (symmetry, transitivity)
		\end{itemize}
	\item class restrictions like: \par \footnotesize \concept{Bottle} $\equiv$
		\concept{Artifact} {\bf that} (\concept{hasShape} {\bf value}
		\concept{cylinderShape})\footnote{This example uses the \emph{Manchester
		syntax}, \url{http://www.w3.org/TR/owl2-manchester-syntax/}} \normalsize
	\item set operations like: \par \footnotesize \concept{Color} $\equiv$ {\bf unionOf}(\concept{blue},
		\concept{green}, \concept{orange}, \concept{black}...) \normalsize
	\item generic SWRL ({\em Semantic Web Rule Language}) rules like: \par
		\footnotesize \concept{looksAt(?agt, ?obj)} $\land$
		\concept{pointsAt(?agt,?obj)} \par $\Rightarrow$ \concept{focusesOn(?agt, ?obj)}
		\normalsize 
	\end{itemize}

We provide in ORO accessors to query, add or remove all these properties and
restrictions (except the SWRL rules) at run-time. This allows
knowledge introspection and enables the robot to alter its own knowledge
structures (the so-called \emph{T-Box} model) during its life-time by adding
new constraints and properties to classes and predicates.

\subsection{Representation of Alternative Cognitive Models}
\label{alterite}

As pictured in Figure~\ref{fig|oro-overview}, ORO stores independent cognitive
models for each agent it interacts with. When ORO actually identifies a new
agent (or infers that some instance is an agent), it automatically creates a
new, separate, in-memory OWL model for that agent. Thus, different robot
components, like supervision or situation assessment, may then store the
agents' beliefs in separate models. All knowledge processing functions in the
robot's primary model are equally available in every agent's model, which
allows us to store and reason on different (and possibly globally inconsistent)
models of the world.

Each of these models is independent and logically consistent,
enabling reasoning on different perspectives of the world that would otherwise
be considered as globally inconsistent (for instance, an object can be visible for the
robot but not for the human. This object can have at the same time the property
\concept{isVisible \textbf{true}} and \concept{isVisible \textbf{false}} in
two different models). 

This feature actually allows us to consider the robot to be endowed with a
\emph{theory of mind}~\cite{Scassellati2002}: the robot can explicitly model
the belief state of its interactors, opening new possibilities for the control
architecture. In section~\ref{exp:disambVPT} we present an example of how we use this feature to
make sense of user sentences from his/her point of view. Moreover,
these multiple models can be viewed as different interpretive scopes,
allowing the robot to interpret the same reality from different points of view.

% \subsection{Categorization}
% \label{categorization}
% 
% We have implemented several algorithms (common ancestors, computation of the
% best discriminant~\cite{Ros2010b}) to help the robot cluster a set of
% concepts based on their symbolic similarities. One particular application of these functions is discrimination.
% While interacting with a user, the robot quite often needs to clarify an
% ambiguity produced by its human partner. For instance, a user may refer to a
% ``bottle'' where two bottles are currently visible. Discrimination routines
% can identify possible (symbolic) differences (\eg the color or the size of the
% bottles) that permit the robot to ask an accurate question to the user in order
% to solve the ambiguity. This discrimination can occur from the robot's
% perspective or from a specific agent's perspective. Usage of these
% categorization abilities are illustrated in Sections~\ref{dialogs:disamb} and~\ref{spygame}.
% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{First Application: Goal and Plan Management}
\label{sec:plan}

\fxfatal[author=Severin]{Reformulate this section to put emphasis on 'How we
use the whole architecture to make symbolic planning grounded into the real
physical world.'}

The Goal and Plan Management activity involves the human-aware
symbolic task planner component and the dedicated robot controller
activity (Figure~\ref{architecture_fg}).

\subsection{Symbolic Task Planning}

In order to devise how a given goal can be accomplished, the robot has
to elaborate a plan,~\textit{i.e.} a set of actions to be achieved by
the robot and its human partners.  This is the role of a HATP
\cite{Alili2008} (for Human Aware Task Planner).  HATP is based on a
Hierarchical Task Network (HTN) refinement which performs an iterative
task decomposition into sub-tasks until reaching atomic
actions~\cite{Nau2003}.  The planning domain defines a set of methods
describing how to decompose a task and can be seen as the Howto
knowledge of the robot.  HATP is able to produce plans for the robot's
actions as well as for the other participants (humans or robots). It
can be tuned by setting up different costs depending on the actions to
apply and by taking into account a set of constraints called social
rules. This tuning aims at adapting the robot's behavior according to
the desired level of cooperation of the robot.

%\subsubsection*{Agents and action streams:}
\vspace{0.3cm}
\noindent
\textbf{Agents and action streams:}
The robot plans not only for itself but also for the other agents. The
resulting plan, called ``shared plan'' is a set of actions that form
a stream for each agent involved in the goal achievement. Depending on
the context, some ``shared plans'' contain causal relations between
agents. For example, the second agent needs to wait for the success of
the first agent's action to be able to start its own action. When the
plan is performed, causal links induce synchronization between
agents. Figure~\ref{plan_hatp1} illustrates a plan with two streams.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\columnwidth]{./figs/plan1.pdf}
  \caption{A plan produced by HATP with 2 streams}
  \label{plan_hatp1}
\end{figure}

%\subsubsection*{Action costs and social rules:}
\vspace{0.3cm}
\noindent
\textbf{Action costs and social rules:}
A cost and a duration function is associated to each action.
The duration function provides a duration interval for the action
achievement and is used, in one hand, to schedule the different
streams and, in the other hand, as an additional cost function.
In addition to these costs, HATP also takes into account a set of social
rules.  Social rules are constraints aiming at leading the plan
construction towards the best plan according to some human
preferences. The social rules we have defined so far deal with:

\begin{itemize}
\item undesirable state: to avoid a state in which the human could
  feel uncomfortable;
\item undesirable sequence: to eliminate sequences of actions that can
  be misinterpreted by the human;
\item effort balancing: to adjust the work effort of the agents;
\item wasted time: used to avoid long delays between the actions of
  the human partner;
\item intricate links: to limit dependencies between the actions of
  two or more agents.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\columnwidth]{./figs/plan2.pdf}
  \caption{A plan with the wasted time social rule}
  \label{plan_hatp2}
\end{figure}

Figure~\ref{plan_hatp2} illustrates an alternative plan to the previous 
one (Figure~\ref{plan_hatp1}) if the wasted time social rule is used.
The obtained shared plan is the best plan according to a global evaluation of
these multiple criteria.

%\subsubsection*{Several levels of cooperation:} 
\vspace{0.3cm}
\noindent
\textbf{Several levels of cooperation:} 
By tuning its costs
and adapting its social rules, HATP can be used to compute various
alternative plans. These plans can be categorized into several levels
of cooperation

\begin{itemize}
\item helping the human to achieve his goal by acting for him
\item sharing concrete resources by handing some objects
\item collaboration of the robot and the human by coordinating their
  actions towards a human-robot joint goal.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Second Application: Natural Language Grounding}
\label{dialog}


Verbal interaction with human presents two categories of challenges: syntactic
ones, and semantic ones. The robot must be able to process and analyze the
structure of human utterances, \ie natural language sentences, and then make
sense of them. 

In this second application of our cognitive architecture, we show how we
harness the symbolic knowledge base to ground, interpret and act on verbal
instructions.

Furthermore, this process takes full advantage of the embodied nature of the
interaction: because deitic gestures and postures are also available in the
symbolic knowledge base (as presented in section~\ref{spark-deitic}) and
dynamically updated during the interactions, verbal dialogue processing turns
into a truely multi-modal communication processing, leading to more robust
interpretation.

We present here the processing of three categories of sentences:
\emph{statements}, \emph{desires} and \emph{questions} that can be answered
from the declarative knowledge present in the robot knowledge base (a choice
similar to the \emph{Behaviour Cycle} in the GLAIR
architecture~\cite{Shapiro2009}). In our work, the grounding process of the
human discourse consists in extracting either the \emph{informational} content
of the sentence to produce statements or its \emph{intentional} content (\ie
performative value) to collect orders and questions. We focus this application on the grounding (we call it
\emph{resolution}) of concepts involved in the human discourse through the
robot's own knowledge: refer to Kruijff et al.~\cite{Kruijff2010}
for a broader survey of formal approaches to natural language processing in the
robotics field.

\begin{figure}[!t]
\centering
  \includegraphics[width=0.7\linewidth]{figs/dialog_module_simple.pdf}
  \caption{The {\sc Dialogs} module has three main steps: the parsing,
  the interpretation and the verbalization. The interpretation module is
  responsible for both the \emph{resolution} and the semantic content
  \emph{analysis and translation}.} 
  \label{fig|dialog}
\end{figure}

Our dialogue processing is done by a module called {\sc Dialogs} (Figure~\ref{fig|dialog}) that processes human
input in natural language, grounds the concepts in the robot's knowledge and
eventually translates the discourse in a set of queries or declarative OWL/RDF
statements.  

The user's input is first pre-processed. For instance, \emph{I'm} constructs are expanded
into \emph{I am} and then parsed by a rule-based (\ie
grammar-free) tool that extracts the grammatical structure from the user's
sentence. Figure~\ref{dialog|parser_output} shows an example of the raw output of the parser for a moderately complex sentence.

\begin{figure}%[!ht]
\begin{center}
\scriptsize
\begin{alltt}
>> IMPERATIVE
VP: \textbf{remember} (present simple)
    SUBSENTENCE (aim: that)
      NP: \textbf{I}
      VP: \textbf{want} (present simple)
        direct objects: 
          NP: \textbf{you}
        secondary VP: \textbf{give} ()
              direct objects:
                NP: my \emph{nice blue} \textbf{bottle}
              indirect objects:
                NP: \textbf{me}
\end{alltt}
\end{center}
\caption{Raw output of the {\sc Dialogs} parser after processing the
sentence: ``remember that I want you to give me my nice blue bottle.'' 
Nominal groups are not grounded yet.} 
\label{dialog|parser_output}
\end{figure}

The output of the parser is then sent to the \emph{interpretation} module, the
core of the component.  Interpretation consists in three distinct operations:
the sentence \emph{resolution} (concepts grounding), the \emph{content
analysis} (what is the intent of the utterance: information, question or
desire) and the \emph{statement building} (translation into RDF statements).

The sentence resolution has three steps: {\it(i)} pronouns and anaphora are
replaced by the correct speaker ID and the ID of the last object referred to
(extracted from the dialogue history) respectively, {\it(ii)} nominal groups are
disambiguated and grounded (noun phrase resolution), and {\it(iii)}
verbal groups are resolved and their associated \emph{thematic roles} are
retrieved (verb phrase resolution).

\small
\begin{pseudocode}[ruled]{Resolution}{sentence, currentSpeaker}
\label{algo|Resolution}

\mathcal{G} \GETS \CALL{ParseNominalGroups}{sentence} \\

\FOREACH g \in \mathcal{G} \DO 
\BEGIN
   \mathcal{D} \GETS \CALL{GenerateDescription}{g} \STMTNUM{5.1em}{res.desc}\\
   candidates \GETS \CALL{Ontology.Find}{\mathcal{D}} \STMTNUM{4em}{res.onto}\\
   
   \IF \left|{candidates}\right| = 0 \THEN
    \BEGIN
      \OUTPUT{\mbox{Couldn't resolve the group!}} \\
      \EXIT \\
    \END
   \ELSEIF \left|{candidates}\right| = 1 \THEN
      id \GETS candidates[0]\\

   \ELSE
      \BEGIN
	\IF \CALL{Ontology.CheckEquivalent}{candidates} \THEN
	  id \GETS candidates[0] \\
	\ELSE
	  id \GETS \CALL{Discrimination}{candidates} \STMTNUM{0em}{st.discrimination}\\
      \END \\
   \CALL{Replace}{g, id, sentence}
\END
\end{pseudocode}
\normalsize

As represented in Figure~\ref{fig|dialog}, interpretation tightly relies on the
communication with the knowledge base. All the concepts the robot manipulates
are stored in the ontology server and retrieved through logical
queries, except for the verbs that are currently stored in a dedicated library
(the \emph{action library} in the diagram).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Technical analysis}
%\label{examples}

In order to better understand the overall process of the {\sc Dialogs} module 
and its relation with ORO, we next describe the different steps of the approach 
based on three examples. In these examples we assume that some initial facts 
are present in the knowledge base (section~\ref{modeling_real_world} discusses how 
the initial knowledge can be acquired), both in the robot's own model and in the human's model.  Since the robot tries to ground a human utterance, all queries are sent to the human model in order to interpret it from the human perspective. 

\subsection{Informational Content Extraction}
\label{informational_content_extraction}

%\begin{figure}
%    \centering
%	\begin{tabular}{p{7cm}}
%	\emph{Initial knowledge model of} \texttt{human\_01}\\
%	\hline
%    	\hspace{0.3cm}\stmt{banana\_01 type Banana} \\
%    	\hspace{0.3cm}\stmt{banana\_01 hasColor yellow}\\
%	
%	\vspace{0.5em}
%	\emph{Human input}\\
%	\hline
%	\hspace{0.3cm}``The yellow banana is big!'' \\
%
%	\vspace{0.5em}
%	\emph{Generated partial statements}\\
%	\hline
%	\hspace{0.3cm}\stmt{?obj type Banana} \\
%    	\hspace{0.3cm}\stmt{?obj hasColor yellow} \\
%    	\hspace{0.7cm}$\Rightarrow$ \concept{?obj = banana\_01}\\
%
%	\vspace{0.5em}
%	\emph{Newly created statements}\\
%	\hline
%	\hspace{0.3cm}\stmt{banana\_01 hasSize big} \\
%	\end{tabular}
%\caption{First example of natural language grounding: the nominal group ``the
%yellow banana'' is matched with the individual \concept{banana\_01}.
%``$\Rightarrow$'' represents the output of the ontology server.}
%\label{dialog|ex1}
%\end{figure}

\begin{figure}
    \centering
%	\begin{tabular}{p{0.5\columnwidth} | p{0.5\columnwidth}}}
	\begin{tabular}{l|l}
	\emph{Initial knowledge} \texttt{human\_01} &
	\emph{Human input}\\	
	
	\hline

    	\stmt{banana\_01 type Banana} &
	``The yellow banana is big!'' \\
	
    	\stmt{banana\_01 hasColor yellow} & \\
	\vspace{0.5em}\\
	\hline

	\emph{Generated partial statements} &
	\emph{Newly created statements}\\
	\hline

	\stmt{?obj type Banana} &
	\hspace{0.2cm}\stmt{banana\_01 hasSize big} \\
	
    	\stmt{?obj hasColor yellow} & \\
    	\hspace{0.2cm}$\Rightarrow$ \concept{?obj = banana\_01}\\

	\hline
	\end{tabular}
\caption{First example: content extraction.
``$\Rightarrow$'' represents the output of the ontology server.}
\label{dialog|ex1}
\end{figure}


Figure~\ref{dialog|ex1} shows a first example of human discourse grounding and
the extraction of informational content. We assume that the robot knowledge
base only contains two initial statements in the human model. The user
asserts a new one: ``The yellow banana is big!''. 
We first want to match the nominal group \emph{The yellow banana} to an already
known concept (algorithm~\ref{algo|Resolution}), and second to translate the property \emph{is big} into a predicate ({\tt hasSize}) to state its semantics. 

To resolve the nominal group \emph{The yellow banana} a set of partial
statements that describe the concept is generated based on the grammatical
parsing of the sentence (algorithm~\ref{algo|Resolution}(\ref{res.desc})). The parsed tree of each nominal group is translated into statements based on a set of rules.
In the example, a banana (\stmt{?obj type Banana})
that is yellow (\stmt{?obj hasColor yellow})\footnote{Predicates like
\concept{hasColor} or \concept{hasSize} that bind \concept{banana\_01} to
adjectives are extracted from a predefined database of $[Predicate \rightarrow
AdjectiveCategory]$, and falls back on the generic \concept{hasFeature}
predicate if the adjective is not known.}.  Based on these partial statements a
SPARQL query is sent to the ontology server to retrieve possible instances that match
the description (algorithm~\ref{algo|Resolution}(\ref{res.onto})).

In this first simple case, the concept \concept{banana\_01} is unambiguously
matched (since there is only one possible banana) and returned. Finally, we can
now add the new information provided by the human, \ie the new statement
\stmt{banana\_01 hasSize big}, to the human model in the ontology server.

% The translation of \emph{yellow} to \stmt{hasColor yellow} is not obvious: in
% the general case, we associate a adjective to the noun it characterizes with
% the \concept{hasFeature} predicate (for instance, \emph{The sight is beautiful}
% would translate to \stmt{sight hasFeature beautiful}). But we can also manually
% set the predicate associated to a category of adjectives: It is what has been
% done for the main colours. Another example is the size: for known size
% adjectives (big, small, etc.), the \concept{hasSize} predicate is being used.


\subsection{Intentional Content Through Verb Resolution}
The sentence in the first example is built with the state verb \emph{be} at
indicative. Let us examine a different example with an action verb at
imperative mode (an order): ``Give me the banana". The process is
described in Figure~\ref{dialog|ex2}.

%\begin{figure}
%    \centering
%	\begin{tabular}{p{7cm}}
%	\emph{Initial knowledge model of} \texttt{human\_01}\\
%	\hline
%    	\hspace{0.3cm}\stmt{banana\_01 type Banana} \\
%    	\hspace{0.3cm}\stmt{banana\_01 hasColor yellow}\\
%	\end{tabular} \\
%
%	\vspace{0.5em}
%
%	\begin{tabular}{p{7cm}}
%	\emph{Human input}\\
%	\hline
%    	\hspace{0.3cm}``Give me the banana.'' \\
%	\end{tabular} \\
%
%	\vspace{0.5em}
%
%	\begin{tabular}{p{7cm}}
%	\emph{Generated partial statements}\\
%	\hline
%    	\hspace{0.3cm}\stmt{?obj type Banana} \\
%	\hspace{0.7cm}$\Rightarrow$ \concept{?obj = banana\_01}\\
%
%	\end{tabular} \\
%
%	\vspace{0.5em}
%
%	\begin{tabular}{p{7cm}}
%	\emph{Newly created statements}\\
%	\hline
%    	\hspace{0.3cm}\stmt{human\_01 desires situation\_a3f74} \\
%    	\hspace{0.3cm}\stmt{situation\_a3f74 type Give} \\
%    	\hspace{0.3cm}\stmt{situation\_a3f74 performedBy myself} \\
%    	\hspace{0.3cm}\stmt{situation\_a3f74 actsOnObject banana\_01} \\
%    	\hspace{0.3cm}\stmt{situation\_a3f74 receivedBy human\_01} \\
%	\end{tabular}
%
%\caption{Second example: processing an order.}
%\label{dialog|ex2}
%\end{figure}

\begin{figure}
    \centering
	\begin{tabular}{l|l}
	\emph{Initial knowledge} \texttt{human\_01} &
	\emph{Human input}\\
	
	\hline
	
    	\stmt{banana\_01 type Banana} &
	``Give me the banana.'' \\
	
    	\stmt{banana\_01 hasColor yellow} & \\
	\vspace{0.5em}\\
	\hline
    	
	\emph{Generated partial statements} &
	\emph{Newly created statements}\\
	\hline
    	\stmt{?obj type Banana} & 
	\stmt{human\_01 desires sit\_a3} \\
	
	\hspace{0.2cm}$\Rightarrow$ \concept{?obj = banana\_01}
    	& \stmt{sit\_a3 performedBy myself} \\
    	& \stmt{sit\_a3 actsOnObject banana\_01} \\
    	& \stmt{sit\_a3 receivedBy human\_01} \\
	\end{tabular}

\caption{Second example: processing an order.}
\label{dialog|ex2}
\end{figure}


\label{processing_of_actions}

In order to capture the intentional content of a sentence (for example, an
order) we need to retain the semantics of the verb and its complements.
\emph{Thematic roles} allow for semantically linking a verb to its complements.  
There is no general agreement amongst linguists on a comprehensive list of 
thematic roles. The amount and the granularity of roles varies a lot in 
the literature~\cite{Gutierrez2001}. We thus use a small set of them, which matches
the relations the robot can actually achieve (we discuss
possible extensions in the conclusion). For instance, in the second example,
the verb \emph{give} has three thematic roles: \concept{performedBy},
\concept{actsOnObject} and \concept{receivedBy}.

The list of actions the robot can plan for (currently \emph{take},
\emph{place}, \emph{give}, \emph{show}, \emph{hide} and \emph{move}) along with
possible synonyms (for example, \emph{to pick} is set as a synonym of \emph{to
take}) and their associated thematic roles are stored in a predefined library
of actions. For each action we identify and store: the role of the subject in
the sentence (always \concept{performedBy}); the role of the direct object
(for instance, \concept{actsOnObject}); and the role of each of the indirect
objects with their optional prepositions (for instance,
\concept{receivedBy})\footnote{Note that in example 2, ``give me the banana'',
the pronoun ``me'' appears before ``banana'', while it is an indirect
complement --- ``give it {\bf to me}''. The parser correctly handles these
cases.}. Moreover, through the ontology we check that each holder of a
role is semantically consistent. For instance, the action \emph{Give} must have a
manipulable physical item (\concept{Artifact}) as direct object. Thus, if the
concept the robot finds for the thematic role \concept{actsOnObject} cannot be
inferred to be an artifact, the robot goes back to the human saying it does not
understand.

This second example  also shows the pronoun reference resolution: ``me'' is
replaced by the id of the current speaker, while ``you'' is replaced by
\concept{myself} (\concept{myself} always represents the robot itself). When
present, anaphoras (references to previous concepts like ``give me the banana, I
like {\bf it}.'') are also resolved in the same step.

Once the sentence is completely resolved and translated into a formal
representation (a human desire in this example\footnote{Orders are here
represented as human desires: the human desires a specific new situation.}), we
store it in the ontology server. The robot's decisional/executive layers can
then decide whether to execute the order or not. 

\subsection{Informational Content Extraction Requiring Clarification}
\label{dialogs:disamb}
\begin{figure}
    \centering
	\begin{tabular}{p{7cm}}
	\emph{Initial knowledge model of} \texttt{human\_01}\\
	\hline
     	\hspace{0.3cm}\stmt{banana\_01 type Banana} \\
     	\hspace{0.3cm}\stmt{banana\_01 hasColor yellow} \\
     	\hspace{0.3cm}\stmt{banana\_02 type Banana} \\
     	\hspace{0.3cm}\stmt{banana\_02 hasColor green} \\
	\end{tabular} \\

	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Human input}\\
	\hline
     	\hspace{0.3cm}``The banana is good.'' \\
	\end{tabular} \\

	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Generated partial statements}\\
	\hline
     	\hspace{0.3cm}\stmt{?obj type Banana} \\
	\hspace{0.7cm} $\Rightarrow$ \concept{?obj = [banana\_01, banana\_02]}
	\end{tabular} \\

	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Discrimination process}\\
	\hline
     	\hspace{0.3cm}\concept{discriminate([banana\_01, banana\_02])} \\
	\hspace{0.7cm} $\Rightarrow$ \concept{?hasColor = [yellow, green]}
	\end{tabular} \\

	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Robot output speech}\\
	\hline
     	\hspace{0.3cm}``The yellow one or the green one?'' \\
	\end{tabular} \\

	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Human answer}\\
	\hline
     	\hspace{0.3cm}``The green one.'' \\
	\end{tabular} \\
    
	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Extended human input}\\
	\hline
     	\hspace{0.3cm}``The green banana is good.'' \\
	\end{tabular} \\
	
	\vspace{0.5em}

	\begin{tabular}{p{7cm}}
	\emph{Generated partial statements}\\
	\hline
     	\hspace{0.3cm}\stmt{?obj type Banana} \\
     	\hspace{0.3cm}\stmt{?obj hasColor green} \\
	\hspace{0.7cm} $\Rightarrow$ \concept{?obj = [banana\_02]}
	\end{tabular} \\
    
	\vspace{0.5em}
	\begin{tabular}{p{7cm}}
	\emph{Newly created statements}\\
	\hline
     	\hspace{0.3cm}\stmt{banana\_02 hasFeature good} \\
	\end{tabular}

\caption{Ambiguity resolution: in this example, ``banana'' can refer to the
yellow banana (\concept{banana\_01}) or the green one (\concept{banana\_02}).
Discrimination routines handle the disambiguation process.} \label{dialog|ex3}
\end{figure}

This last example (Figure~\ref{dialog|ex3}) shows the resolution of ambiguous
concepts. In this case the user refers to ``the banana'' while two instances of 
the \concept{Banana} class exist in the ontology. The robot needs to find out
to which instance the user is actually referring to. To this end,
disambiguation routines (algorithm~\ref{algo|Resolution}(\ref{st.discrimination}), see \cite{Ros2010b} for details of the routines)
find differences between the instances (in the example, one banana is yellow
while the other one is green) and build a sentence through the
\emph{verbalization} module to ask the user a closed question that will
help clarify the ambiguity: ``Is it yellow or green?'' The user's answer is
parsed and merged with the previous sentence. The resulting, augmented,
sentence (``The green banana is good") goes again through all the
interpretation steps. This process is repeated until no ambiguities arise. 
In the example, the \concept{banana\_02} is finally returned.

%If no differences \fxfatal{should we say 'in the human model', even if 
%getDiscriminantForAgent currently doesn't work?} can be found, an open question 
%(``give me more information'') is send to the human.

Several other strategies are used in parallel to disambiguate concepts without
having to ask for more information to the human:

\begin{itemize}
	\item Which objects are currently visible to the human? If only one of
	them, then it is probably the one the user is talking about. 
	\item Did a previous interaction involved a specific object that would
	still be the subject of the current sentence?
	\item Is the user looking or pointing at a specific object?
\end{itemize}

%Two cases can alter the way the discrimination routines work:
%\begin{enumerate}
%    \item If a sentence starts with {\it Learn that...}, failures during 
%    discrimination are interpreted as new concepts, and instead of marking the 
%    nominal as not resolved, and new identifier is created and add to the knowledge base.
%    \item For questions like {\it Which color is the bottle?}, the discrimination 
%    algorithm can not use the feature {\it color} to identify to bottle. The 
%    resolution algorithm pass this kind of constraints as a parameter of the 
%    discrimination routines.
%\end{enumerate}

 
While no examples involving questions have been detailed, factual \emph{wh-}
questions and polar (\emph{yes/no}) questions can be processed in a similar way
by \textsc{Dialogs}. For instance, a question like ``What is on the table?'' is
grounded (to extract the relation \concept{isOn} and to find what \emph{table}
refers to) and transformed into the following kind of query: \concept{find ?var
[\stmt{?var isOn table1}]}.  Answers are converted back to a full sentence by
the \emph{verbalization} module, and uttered to the human.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%\section{Experiments}
%\label{experiments}

%This section presents several experiments where the different features of our
%approach are represented. The experiments have been conducted on two different
%platforms: the \emph{Rosie} manipulator from the Technical Universtiy of
%Munich, a dual-arm, holonomic service robot, running the
%ROS\footnote{\emph{Robotic Operating System}, \url{http://www.ros.org}}
%middleware; and the \emph{Jido} robot~\cite{Alami1998a} from LAAS-CNRS, a
%similar, single-arm, service robot, running the LAAS's {\sc Genom/pocolibs}
%stack. Both platforms share the use of ORO, the ontology server described in
%this work.

%\subsection{\emph{Naming} Experiment}
%\label{naming}

%\begin{figure}
%\centering
%\includegraphics[width=0.45\columnwidth]{figs/kimp1.jpg}
%\includegraphics[width=0.45\columnwidth]{figs/rviz.png}
%%\parbox[c]{4.3cm}{\includegraphics[width=0.5\columnwidth]{figs/kimp1.jpg}}
%%\parbox[c]{4.1cm}{\includegraphics[width=4cm]{figs/rviz.png}}
%\flushleft
%\hspace{1.8cm} (a) \hspace{3.9cm} (b)
%\caption{(a) Rosie, looking for objects it may know, and (b) view from Willow
%Garage's RViz. The clusters of point are given a unique identifier by the
%perception module. This allows the supervision module to create the link between the physical
%objects and their symbolic representation in ORO.} 
%\label{fig|kimpwatching}
%\end{figure}

%The \emph{Naming} task uses ORO to anchor perception into the robot's knowledge
%through interaction with the user. This task has been implemented on the Rosie
%robot at TU Munich.

%The robot selects an unknown object from the table, shows it to the user, and
%asks about its name and type (Figure~\ref{fig|kimpwatching}). The user
%interacts with the robot through a dedicated bridge, allowing him/her to chat with
%the robot with a standard Jabber instant messaging client.
%Figure~\ref{tab|transcript_kimp} shows a chat session with Rosie performing the
%naming experiment. The human describes the object until the robot recognizes a
%type it already knows. More precisely, the robot recursively asks the human for the categories the object belongs to until reaching a known one. At this point, the robot is able to connect the new object information to already known concepts. Therefore, the robot
%accurately anchors perception in its symbolic model and it is able to reason
%about it.  The learning process starts over again with each unknown object. To
%further exploit the knowledge reasoner in this experiment, we ask the robot
%which object does not belong to a typical breakfast table, \ie we ask for the odd
%objects. In this case, it corresponds to objects that are neither of type
%\concept{Tableware} nor \concept{Food}. 

%The supervision module (\textsc{cram}\footnote{\textsc{cram} (Cognitive Robotic
%Abstract Machine) is a RPL-derived framework for rapid development of cognitive
%robot control programs we currently develop.}~\cite{Beetz2010}) automatically
%updates the ORO server whenever an object enters or leaves the field of view.
%%(the perception is based on the \textsc{CoP} framework~\cite{Klank2009}). 
%Therefore, the integration of ORO can be seen as an extension of
%the robot's belief state that not only contains abstract identifiers
%of the internal object representation used in plans, but also the
%semantics and roles of objects in the scenario.

%\begin{figure}%[!ht]
%\centering
	%\begin{tabular}{r|p{7cm}}
	%\chatN{rosie} & Servus! The Bavarian robot is up and fit! \\
	%\chatN{rosie} & What is the name of the white cluster on the table? \\
	%\chatN{human} & ``Rosie's mug'' \\
	%\chatN{rosie} &\emph{[\stmt{obj\_01 label "Rosie's mug"} added]}\\
				%& What kind of object is ``Rosie's mug''? \\
	%\chatN{human} & Mug \\
	%\chatN{rosie} & \emph{[\stmt{obj\_01 type Mug} added]}\\
			%& What is a Mug?\\ 
	%\chatN{human} & Tableware\\
	%\chatN{rosie} & \emph{[\stmt{Mug subClassOf Tableware} added. Rosie already
	%knows about \emph{Tableware} (it belongs to the common-sense ontology), so
	%the description of this object ends.]}\\
	%& (...) \\
	%\chatN{rosie} &  What is the name of the black cluster on the table? \\
	%\chatN{human} & ``my\_laptop'' \\
	%\chatN{rosie} & What kind of object is ``my\_laptop''? \\
	%\chatN{human} & Computer \\
	%\chatN{rosie} & What is a Computer? \\
	%\chatN{human} & Artifact \\
	%\chatN{rosie} & Do you want me to learn more objects? \\
	%\chatN{human} & no \\
	%\chatN{rosie} & These objects are odd: ``my\_laptop'' \\
	%\end{tabular}
	%\caption{Transcript of a Jabber session with the robot Rosie.}
	%\label{tab|transcript_kimp}
%\end{figure}

%% An unexpected example of what the symbolic reasoning layer brings to more
%% traditional robotic architectures emerged during the \emph{Naming} experiment:
%% the perception routines provided segmented blobs corresponding to objects along
%% with their colors to update the information in ORO. At some point, ORO suddenly
%% refused to add an object. What seemed at first a communication bug between
%% modules, was actually the consequence of a consistency check by ORO: because of
%% bad light conditions, the color recognition was not very reliable, and the same
%% object was set to have two different colors at the same time. Since the
%% \concept{hasColor} predicate we had decided to use was a functional predicate
%% (\ie objects can have only one color), ORO inferred an inconsistency and
%% discarded the new fact. This kind of logical failure can be used to improve
%% low-level perception outcomes by ``closing the loop'' with high-level, symbolic
%% knowledge.

%\subsection{\emph{Spy Game} Experiment}
%\label{spygame}

%This game is based on the traditional children game ``I Spy''. The idea is to
%discover the object or concept one of the participants is thinking of by asking
%questions such as: ``Is it green? Is it a machine? Is it on your left?'', etc.
%When playing, children exploit their knowledge about the world while
%categorizing and describing objects through useful discriminants that will
%allow them to find out the answer as fast as possible while including
%perspective taking abilities~\cite{Moll2006}.

%\begin{figure}
%\centering
%\includegraphics[width=0.45\columnwidth]{figs/spy-game-real.jpg}
%\includegraphics[width=0.45\columnwidth]{figs/spy-game-mhp.jpg}
%%\parbox[c]{4.3cm}{\includegraphics[width=4.3cm]{figs/spy-game-real.jpg}} \hspace{0.2em}
%%\parbox[c]{4.1cm}{\includegraphics[width=4.1cm]{figs/spy-game-mhp.jpg}}
%\flushleft
%\hspace{1.8cm} (a) \hspace{3.9cm} (b)
%\caption{Spy game scenario: (a) Real environment and (b) 3D environment model, viewed in \textsc{SPARK}.}
%\label{fig|spyGameScenario}
%\end{figure}

%The scenario for this game (Figure~\ref{fig|spyGameScenario}) consists on a
%face-to-face interaction where the human thinks of an object present in the
%environment, while the robot queries the human until either discovering the
%object or giving up~\cite{Ros2010a}. The game starts with the human user giving a
%first hint (communication is done through a keyboard and screen), allowing the
%robot to start the search filtering those objects that fulfill this first
%description. Based on this subset, ORO provides a descriptor (or set of
%descriptors) that allows maximum discrimination among objects in the subset.
%The robot queries the user about the value of the descriptor (or the most
%discriminant among the set of descriptors) and with this new information, the
%current subset of objects is filtered again. The process is repeated until
%either obtaining a single object that fulfills all the descriptor values, or
%failing (\ie no object found). 

%%\begin{figure}%[!ht]
%%\centering
%%\begin{scriptsize}
%%\begin{tikzpicture} %[level/.style={sibling distance=30mm/#1},scale=0.8]
%%	%[edge from parent fork down,
%%	%every node/.style={fill=black!30,rounded corners},
%%	%[parent anchor=east,child anchor=west,grow=east,
%%	%edge from parent/.style={thick,draw}]
%%	\node {Artifact}
%%	child {node {Tableware}
%%		child {node {Bottle}}
%%		child {node {Cup}}
%%		}
%%	child {node {GameObject}}
%%	child {node {Furniture}
%%			child {node {Table}}
%%			child {node {Chair}}
%%			child {node {Shelf}}};
%%\end{tikzpicture}
%%\end{scriptsize}
%%\caption{Example of object categorization used in the scenario.}
%%\label{fig|objectsSpyGame}			
%%\end{figure}

%%Thing
%%Object
%%Tableware
%%	- bottle: red, blue, yellow orange
%%	- cup: white
%%Furniture
%%	- table: hrp2table, lowtable
%%	- chair: chair1 and chair2
%%	- shelf: pink_shelf
%%GameObject: spacenavbox, orangebox, accesskit

%We have integrated the game on the LAAS-CNRS Jido robot.
%Objects are identified through a tag-based vision approach\footnote{ARToolKit:
%\url{http://www.hitl.washington.edu/artoolkit/}} and motion capture is used for
%human tracking. Their descriptions regarding categories (type of object) and
%features (color, shape) are manually given in advance. Spatial relationships
%(\concept{front}, \concept{back}, \concept{left}, etc, and \concept{in},
%\concept{on} and \concept{next to}) and visibility (only visible objects for
%both agents can be considered in the game) are automatically computed on-line
%by \textsc{SPARK}. Figure~\ref{fig|spyGameExample}
%shows an example of a round game.

%\begin{figure}
%\centering
	%\begin{tabular}{r|p{6cm}}
		%\chatN{human} & It is a tableware.\\
		%\chatN{jido} & \emph{[retrieve possible objects: coke-bottle, yellow-bottle, orange-bottle, cup-with-handle]} \\
	 			%& \emph{[keep visible objects: coke-bottle, yellow-bottle, cup-with-handle]}\\
				%& \emph{[obtain discriminants: type, color.]}\\
				%& Which type of object it is: bottle or cup? \\
		%\chatN{human} & Bottle.\\
		%\chatN{jido} & \emph{[obtain possible objects: coke-bottle, yellow-bottle.]}\\
				%& \emph{[obtain discriminants: color.]}\\
				%& What color is the object: red or yellow?\\
		%\chatN{human} & Red.\\
		%\chatN{jido} & \emph{[obtains possible objects: coke-bottle.]}\\
				%& The object is the Coke bottle!	
	%\end{tabular}\\
	%\caption{Example of the robot playing Spy game.}
	%\label{fig|spyGameExample}
%\end{figure}

%\subsection{\emph{Moving to London} Scenario}

%In order to illustrate some of different reasoning abilities of the robot presented in this paper, we have designed the following
%daily life situation. Tom and Jerry are moving to London, so they are packing
%things in boxes. The scenario takes places in the living room, where Jido is observing while they move things here and there. To assess the
%reasoning abilities of the robot they ask Jido for information (entered through
%keyboard). Ideally, the robot should perform actions when required (\eg
%hand an object when asking ``give me...''). However, since it is out of the
%scope of this work, we do not include any motion from the robot's side. Similar
%to the \emph{Spy Game} scenario, perception of objects is done through a tag-based
%system and humans are detected through motion capture. We next describe in
%detail three situations where we can follow the internal robot's reasoning and
%the interaction with the users.

%\subsubsection{Initial Situation Assessment}

%\begin{figure*}
%\centering
	%\subfigure[]
	%{\includegraphics[width=0.3\linewidth]{figs/situationAssessment1.png}\label{(a)}}
	%\subfigure[]
	%{\includegraphics[width=0.3\linewidth]{figs/situationAssessment2.png}\label{(b)}}
	%\subfigure[]
  	%{\includegraphics[width=0.3\linewidth]{figs/inTheBox2.jpg}\label{c}}
	%\caption{Situation assessment during initialization (before (a) and after (b) placing the objects on the table). In each image the snapshots correspond to: real environment (top-left sub-image); processed image to identify the tagged objects (bottom-left sub-image); and the 3D environment (right sub-image).(c) Jerry asks Jido for the content of the box while pointing at it.}
	%\label{fig|movingToLondon}
%\end{figure*}

%%\begin{figure}%[!t]
%%\centering
%%  \subfigure[]
%%  {\includegraphics[width=0.95\linewidth]{figs/situationAssessment1.png}\label{a}}
%%  \subfigure[]
%%  {\includegraphics[width=0.95\linewidth]{figs/situationAssessment2.png}\label{b}}
%%	\caption{Situation assessment during initialization: a) before placing the
%%	objects on the table, and b) after placing them. In each image the snapshots
%%	correspond to: real environment (top-left sub-image); processed image to
%%	identify the tagged objects (bottom-left sub-image); and the 3D environment
%%	(right sub-image).}
%%	\label{fig|sitAssessRoman}
%%\end{figure}

%First Tom enters the room with some of the things they need to pack: a
%toolbox and two videos. He leaves one of the video tapes (``Jido-E'') inside one of
%the boxes (the cardboard box), and the other one (``Lord of the Robots'') on the
%table. We next describe how the situation assessment takes place at each step, \ie how the
%ontology is updated with the information obtained from the geometric reasoner
%SPARK.
%The initial information in ORO corresponds to:
%\begin{center}
    %\begin{tabular}{ll}
      %\stmt{table type Table} & \stmt{Tom type Human}\\
      %\stmt{cardBoardBox type Box} & \stmt{Jerry type Human}\\
      %\stmt{toolbox type Box} & \stmt{videoTape1 type VideoTape}\\
      %\stmt{videoTape2 type VideoTape} & \stmt{videoTape2 label "Jido-E"}   
      %\end{tabular}
      %\stmt{videoTape1 label "Lord of the Robots"}
%\end{center}
%\vspace{0.10cm}
	%SPARK detects that there is a cardboard box on the table. It thus sends the fact to ORO:

%\begin{center}
    %\begin{tabular}{l}
      %\stmt{cardBoardBox isOn table}
    %\end{tabular}	             
%\end{center}
%Tom enters carrying several objects (Figure~\ref{fig|movingToLondon}a) and places them around. Then he leaves. (Figure~\ref{fig|movingToLondon}b). The following facts are computed and sent to ORO:
%\begin{center}
    %\begin{tabular}{ll}
      %\stmt{toolbox isOn table} & \stmt{toolbox isNextTo videoTape1}\\
      %\stmt{videoTape1 isOn table} & \stmt{videoTape2 isIn cardBoardBox}
      %\end{tabular}
      %\stmt{videoTape1 isNextTo toolbox}
%\end{center}

%\subsubsection{Implicit Disambiguation Through Visual Perspective Taking}
%\label{exp:disambVPT}

%Tom enters the room again while carrying a big box (Figure~\ref{fig|vpt}). He
%approaches the table and asks Jido to handle him the video tape: ``Jido, can
%you give me the video tape''. The \textsc{Dialogs} module queries the ontology to
%identify the object the human is referring to:
%\begin{center} 
%\stmt{?obj type VideoTape} 
%\end{center}
%There are two video tapes in the scene: one on
%the table, and another one inside the cardboard box. Thus, the knowledge 
%base returns both: 
%\begin{center}
%\hspace{0.7cm}$\Rightarrow$ \concept{?obj = [videoTape1, videoTape2]}
%\end{center}
%However, only one is visible for Tom (the one on the
%table). Thus, although there is an ambiguity from the robot's perspective
%(since it can see both video tapes), based on the perspective of its human
%partner it infers that Tom is referring to the video tape on the table, and not
%the one inside the box which is not visible from his view. Therefore,
%non-visible objects are removed obtaining:

%\begin{center}
%\concept{?obj = [videoTape1]}
%\end{center}

%Since only one object is available now, the robot infers
%that the human refers to it and can eventually execute the command, \ie give
%it to the human. Alternatively, the robot could first verify with the human whether
%that was the object being referred to before proceeding to execute the
%action. Table~\ref{table|ptbeliefs} lists the robot's beliefs about itself and
%its human partner involved in this situation.

%\begin{table}
%\begin{center}
%\begin{tabular}{l}
%\hline
%Robot's beliefs about itself (\emph{robot's model}):\\
%\hline
  %\hspace{0.7cm}\stmt{videoTape1 type VideoTape}\\
  %\hspace{0.7cm}\stmt{videoTape1 isOn table}\\
  %\hspace{0.7cm}\stmt{videoTape1 isVisible \textit{true}}\\
  %\hspace{0.7cm}\stmt{videoTape2 type VideoTape}\\
  %\hspace{0.7cm}\stmt{videoTape2 isIn cardBoardBox}\\
  %\hspace{0.7cm}\stmt{videoTape2 isVisible \textit{true}}\\
%\hline
%\hline
%Robot's beliefs about Tom (\emph{Tom's model}):\\
%\hline
  %\hspace{0.7cm}\stmt{videoTape1 type VideoTape}\\
  %\hspace{0.7cm}\stmt{videoTape1 isOn table}\\
  %\hspace{0.7cm}\stmt{videoTape1 isVisible \textit{true}}\\
  %\hspace{0.7cm}\stmt{videoTape2 type VideoTape}\\
  %\hspace{0.7cm}\stmt{videoTape2 isIn cardBoardBox}\\
  %\hspace{0.7cm}\stmt{videoTape2 isVisible \textit{false}}\\
 %\hline
%\end{tabular}
%\end{center}
%\caption{Robot's beliefs about itself and its human partner.}
%\label{table|ptbeliefs}
%\end{table}

%\subsubsection{Explicit Disambiguation Through Verbal Interaction and Gestures}
%\label{disamb-gestures}

%%\begin{figure}%[!ht]
%%  \centering
%%  \subfigure[]
%%  {\includegraphics[width=0.9\linewidth]{figs/inTheBox1.jpg}\label{a}}
%%  \subfigure[]
%%  {\includegraphics[width=0.9\linewidth]{figs/inTheBox2.jpg}\label{b}}
%%\caption{(a) Jerry asks Jido for the content of the box and (b) points at it.}
%%  \label{fig|pointing}
%%\end{figure}

%In this last situation Jerry enters the
%living room without knowing where Tom had placed the video tapes. So he first
%asks Jido: ``What's in the box?''. Before the robot can answer the question it
%has to figure out which box Jerry is talking about. Similar to the previous
%situation, there are two available boxes: 
%\begin{center}
%\begin{tabular}{l}
%\stmt{?obj type Box}\\
%\hspace{0.7cm}$\Rightarrow$ \concept{?obj = [cardBoardBox, toolbox]}
%\end{tabular}
%\end{center}
%However both are visible and the cognitive ambiguity resolution cannot be
%applied. The only option is to ask Jerry which box he is referring
%to\footnote{Note that Jerry is using the definite article \emph{the}: the robot
%has to determine which box is relevant. {\sc Dialogs} supports other kinds of
%quantification (existantial -- \emph{some} --, universal -- \emph{all} -- , explicit cardinality), as expressed by the combination of definite/indefinite and
%singular/plural forms of articles.}: ``Which box, the toolbox or the cardboard box?''
%Jerry could now simply answer the question. Instead, he decides to point at it
%while indicating: ``That box'' (Figure~\ref{fig|movingToLondon}c). The robot's
%perception identifies the {\tt cardBoardBox} as being pointed at and looked
%at by the human and updates the ontology with this new information using a
%rule available in the commonsense ontology:
%\begin{center}
%\begin{tabular}{r}
%\stmt{Jerry pointsAt carboardBox}, 
%\stmt{Jerry looksAt carboardBox}\\
%$\to$ \stmt{Jerry focusesAt carboardBox}\\
%\end{tabular}
%\end{center}
%In the meantime, the \textsc{Dialogs} module is processing the human verbal input. When trying to resolve the reference ``that'' it is able to merge\footnote{Due to synchronization issues, the user should perform the gesture (pointing at) before answering the robot's question and maintain it until the resolution process takes place.} both sources of information, verbal and gestural, to distinguish the box Jerry refers to:
%\begin{center}
%\begin{tabular}{l}
%\stmt{Jerry focusesAt ?obj}\\
%\hspace{0.7cm}$\Rightarrow$ \concept{?obj = [cardBoardBox]}
%\end{tabular}
%\end{center}

%Finally, \textsc{Dialogs} queries the ontology about the content of the box
%and the question can be answered: ``Jido-E''. Note that the object's label is
%used instead of its ID. This way we enhance interaction using familiar names
%given by the users:
%\begin{center}
%\begin{tabular}{l}
%\stmt{?obj isIn cardBoardBox}\\
%\hspace{0.7cm}$\Rightarrow$ \concept{?obj = videoTape2}\\
%\end{tabular}
%\end{center}

%At this point Jerry wants to know where the other video tape is, and that is exactly
%what he asks Jido: ``And where is the other video tape?''. In this occasion, the
%\textsc{Dialogs} module is able to interpret that Jerry is not referring to the
%video tape which they were just talking about, but to the other one:
%\begin{center}
%\begin{tabular}{l}
%\stmt{?obj type VideoTape}\\
%\stmt{?obj differentFrom videoTape2}\\
%\hspace{0.7cm}$\Rightarrow$ \concept{?obj = [videoTape1]}
%\end{tabular}
%\end{center}

%Since there is only one possible ``other'' video tape (there are only two video tapes in
%the scene), it can directly answer Jerry: ``The other video tape is on the table and
%next to the toolbox.''
%\begin{center}
%\begin{tabular}{l}
%\stmt{videoTape1 isOn table}, 
%\stmt{videoTape1 isNextTo toolbox}
%\end{tabular}
%\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{An illustrative example}\label{sec:expes}

%We assume here that the robot (and the human) has been given the joint goal
%``CLEAN TABLE''. For HATP, this means putting all tapes that are
%currently on the table in the trashbin. Depending on the state of the
%world and agent preferences, different plans are produced.

%Figure~\ref{plan-etat2} shows a plan produced to clean the table based on the initial the initial state given in  \S~\ref{sub:gtrc}.

%\begin{figure*}[thpb]
  %\centering
  %\includegraphics[width=1.0\textwidth]{./figs/plan3.pdf} \\
  %\caption {A plan produced by HATP for clean the table based on the initial state given in \S~\ref{sub:gtrc}}
  %\label{plan-etat2}
%\end{figure*}

%Let us now take a simpler example to illustrate a full run of the
%system. We have only one tape on the table and it is is reachable only
%by the robot while the throw position on top of the trashbin is
%reachable only by the human.

%% \begin{landscape}
%% \begin{figure*}[thpb]
%%   \centering
%% %  \includegraphics[width=1.0\textwidth]{./figs/manip_run.pdf} \\
%%   \includegraphics[width=17cm]{./figs/manip_run.pdf} \\
%% \begin{center}
%%   \caption {An example of a human-robot collaborative goal achievement}
%% \end{center}
%%   \label{manip_run_fg}
%% \end{figure*}
%% \end{landscape}

%\begin{figure*}[thpb]
  %\centering
%\includegraphics[width=1.0\textwidth]{./figs/rotated-rachid_manip_run-v2.pdf} \\
%%  \includegraphics[width=17cm]{./figs/manip_run-vertical.pdf} \\
%\begin{center}
  %\caption {An example of a human-robot collaborative goal achievement}
%\end{center}
  %\label{manip_run_fg}
%\end{figure*}


%%\subsection{successful execution}

%Figure~\ref{manip_run_fg} illustrates the main processes occurring
%during a multi-step human-robot collaborative goal achievement.  The
%plan produced is quite straightforward and is shown in the third row
%called ``Goal and Plan''. It consists in 4 successive actions
%involving the robot and the human. Robot grasps the tape and then
%places it on the table at a position where it is visible and reachable
%for the human. Human then is asked to pick the tape and throw it in
%the trashbin. The first row, named ``Cameras'', shows several
%snapshots corresponding to various execution steps. Snapshot~1
%presents the initial situation. Snapshots~2, 3, 4 and 5 give the state
%after the successive achievement of the four actions in the plan. The
%second row, named ``3D Model'', shows the display of SPARK at the same
%instants. The fourth, row called ``Robot Speech Acts'', gives robot
%speech acts produced along the execution to inform the human partner
%about goal and plan creation and status and to verbalize the actions
%that the human is asked to execute. The fifth row illustrates robot
%knowledge on itself and on the objects. The sixth row illustrates the
%robot knowledge about the human state. The seventh row gives ongoing
%robot action with action preconditions and effects assessment as well
%as motion execution tasks. Motion trajectory typology can be found
%between the 3D Model views. The eighth row gives ongoing human action
%with action preconditions and effects assessment and monitoring
%activity.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Conclusion}
\label{conclusion}

\subsection{Towards an Event-Driven, Knowledge-Oriented Architecture for Personal Robotics}

In this paper, we have studied knowledge streams between three components: {\it(1)}
{\sc ORO}, an ontology-based knowledge server that stores and maintains
classified RDF statements produced by other modules in agent-specific
models and allows information to be easily retrieved, either through queries or
via an event system; {\it(2)} {\sc SPARK}, the grounded, human-aware 3D model
of the environment that performs all the spatial reasoning within our
architecture, including reasoning involving motion planing (to compute
reachability of objects) and perspective taking, and {\it(3)} {\sc Dialogs}, a
natural language processor that performs simple grammatical parsing of English
language, grounds the semantic content of the utterance (if necessary, also
interacts with the user to disambiguate), and eventually generates a RDF
representation of the sentence.

These components, combined with modules dedicated to symbolic supervision and
task planning (these modules are outside of the scope of this article), compose
an architecture that we call \emph{knowledge-oriented}:

\begin{itemize}
\item{Knowledge is explicitly stored in one central and consistent repository
of facts, accessible by all modules.}
\item{Knowledge is represented in a strict formalism (OWL statements) and
with a clearly defined vocabulary (stated in the {\tt commonsense.oro.owl}
ontology).}
\item{The first two points enable both a loosely-coupled
architecture where modules can very easily be removed or replaced by other ones
as long as they share the same semantics (modules are defined by the knowledge
they produce),}
\item{and a \emph{symbolic} reactive, event-driven approach
to supervision. By managing events at the same level as
the reasoner, we take full advantage of the inference abilities of ORO to
trigger events whose \texttt{true} conditions can be inferred.}
\item{Finally, this architecture allows for the combination of very different knowledge
modalities in a single homogeneous environment, bringing mutual benefits to
components. For instance, the dialogue processing module can perfectly run
without any geometric perception, but its disambiguation routines can
transparently benefit from it when available (since richer symbolic
descriptions of objects are then available).}
\end{itemize}

This architecture moves away from standard layered approaches. Interactions
between components are mostly bidirectional and we do not have a concept of
layers of abstraction (we do, however, have access to the lower level modules
of the robot to execute actions, but all cognition-related modules reside at the
same level). This is especially visible for the dialogue input processing. This
component does not simply act as an alternative perceptual input to the
symbolic database, but also actively queries previously acquired knowledge to
disambiguate and validate the newly created symbolic knowledge.


Regarding the anchoring question, this architecture is
bidirectional. The components we described provide a \textit{bottom-up}
grounding process: SPARK and \textsc{Dialogs} constantly build and push new
symbolic contents about the world to ORO where it becomes accessible to
decisional layers. In parallel, ORO relies on reasoning in a \textit{top-down}
way to produce new facts that may trigger in return physical behaviours. 

\subsubsection{Knowledge and Embodiment}

The three experiments that were presented in the paper all illustrate how the
robot makes use of its embodied nature to establish a meaningful communication
with a human. Mainly, because the robot and the human share the same physical
environment and they perceive each other, we are able to create a mutual
context.

Sloman, in~\cite{Sloman2009}, argues however that the strong focus on
embodiment in the robotics community has hindered progress towards natural
human-robot interaction. Our approach has hopefully made clear that, similar to
Beetz et al.~\cite{Beetz2010}, we do not consider embodiment \emph{per se} outside of
a broader symbolic system, \ie our architecture is not bound to the morphology or
the low-level sensori-motor capabilities of a specific agent. 

However, we can build a model of the ``human point of view'' because the robot
perceives the human, and is able to estimate, at least partially, what the
human perceives or not. We infer that a human focuses on some object because
he/she points at it, looks at it, and besides, the object is visible to him.
This relies on the embodied nature of the interaction. In turn, this allows us
to understand the meaning of sentences like ``Give me that''.

We hope that this contribution shows that considering embodiment as the most
challenging and fruitful characteristic of robotics in regards to the whole AI
community does not contradict with a formal, highly symbolic approach of the
representation and decision problems that arise in robotics. 

Let us conclude this article briefly reviewing and linking Roy's list of challenges
for human-robot dialogue with our current approach: 
\begin{itemize}

	\item While more modalities (especially, deictic gestures and social gazes)
	can be added, we have actually proposed a \emph{cross-modal
	representation system}.

	\item One of the main feature of the \textsc{Dialogs} module is its ability
	to interactively ground concepts through disambiguation, bringing the
	ability for the robot to \emph{associate words with perceptual and action
	categories}.

	\item The ORO knowledge base offers some support for the \emph{modeling of
	context}, but a lot remains to be done in this respect.

	\item \emph{Figuring out the right granularity of models} is partially
	solved by supporting both a geometric reasoning level and a purely symbolic
	level. Generally speaking, it appears that complex robotic systems need
	to operate with a dynamic granularity, depending on the task to achieve.

	\item \emph{Temporal modeling} is currently missing in our architecture,
	and symbolic and geometric \emph{planning} is accomplished outside of the
	knowledge representation loop we presented here. We see planning as an
	essential tool to build predictive knowledge, and we are looking into this
	direction.

	\item Since we provide no time management, our system is currently not able
	to \emph{match past (learned) experiences with the current interaction}.
	This ability is obviously a key step for general action recognition, and
	seems of particular importance for the robot to assess the state of the
	interaction with the human.

	\item Finally, Roy mentions \emph{the ability to take into account the
	human perspective}: this is probably our main contribution which we are now
	trying to develop even further towards psychology-inspired experiments.

\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\bibliographystyle{spbasic}
\bibliography{chapter}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\end{document}

